{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2XsGrf4SGApQ"
   },
   "source": [
    "**PRÁCTICA**: implementar una GAN que genere parejas de números que cumplan (x, x^2).\n",
    "\n",
    "**NOTA**: escoger TPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qzapMYChWU-E"
   },
   "source": [
    "# Mi primera GAN\n",
    "\n",
    "En este Notebook vamos a implementar nuestra primera GAN. Recordad la arquitectura:\n",
    "\n",
    "<center><img src=\"https://miro.medium.com/max/2426/1*XKanAdkjQbg1eDDMF2-4ow.png\"></center>\n",
    "\n",
    "Como ya hemos visto, una GAN consta de un generador (*el falsificador)* y un discriminador (*el policía*).\n",
    "\n",
    "Vamos a comenzar con un ejemplo muy sencillo que iremos complicando conforme avancemos. En esta ocasión no vamos a crear dinero, sino que vamos a generar nuevas muestras que encajen en la función $x^2$. Las muestras reales (nuestro dataset) son las que véis dibujadas en la gráfica siguiente:\n",
    "\n",
    "<center><img src=\"http://mathcentral.uregina.ca/QQ/database/QQ.09.06/mike1.1.gif\"></center>\n",
    "\n",
    "Así que lo primero que vamos a hacer es crear una función que nos permita generar dichas muestras:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "LI0-C8N-Y4hF"
   },
   "outputs": [],
   "source": [
    "# importamos lo que vamos a necesitar\n",
    "import numpy as np\n",
    "from numpy.random import rand, randn\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "blfu5NVyYBH7"
   },
   "outputs": [],
   "source": [
    "# generamos muestras reales con sus correspondientes etiquetas (1)\n",
    "def generate_real_samples(n):\n",
    "    # vamos a generar datos entre -0.5 y 0.5\n",
    "    X1 = rand(n) - 0.5\n",
    "    # nuestros datos son x^2\n",
    "    X2 = X1 * X1\n",
    "    # stack arrays\n",
    "    X1 = X1.reshape(n, 1)\n",
    "    X2 = X2.reshape(n, 1)\n",
    "    X = np.hstack((X1, X2))\n",
    "    # generate class labels\n",
    "    y = np.ones((n, 1))\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oMRAW2QRY7c6"
   },
   "outputs": [],
   "source": [
    "# generamos nuestro dataset\n",
    "X, y = generate_real_samples(n=100)\n",
    "# plot samples\n",
    "plt.scatter(X[:, 0], X[:, 1])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OdbPpUXUZ0gN"
   },
   "source": [
    "Nuestra meta en este caso es ser capaces de generar nuevas parejas de datos $(x, x^2)$ lo más similares posibles a los que generaría la función $f(x)=x^2, x \\in [-0.5, 0.5]$.\n",
    "\n",
    "Vamos a crear primero nuestro discriminador. Lo primero es pensar cuales van a ser las entradas y cuales las salidas. Veamos:\n",
    "\n",
    "Las entradas serán parejas $(x, x^2)$ entre -0.5 y 0.5. Por ejemplo: $(0.2, 0.04)$, $(0.4, 0.16)$, $(-0.2, 0.04)$, $(-0.4, -0.16)$, etc.\n",
    "\n",
    "La salida será un valor binario: 1 si la pareja $(x, x^2)$ realmente se parece a la función $f(x)=x^2$, 0 en caso contrario.\n",
    "\n",
    "Vamos a definirlo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8idxYnPYa5_y"
   },
   "outputs": [],
   "source": [
    "# definimos el discriminador\n",
    "def define_discriminator(n_inputs=2):\n",
    "    model = Sequential()\n",
    "    # Nuestro discriminador va a ser muy sencillo: va a tener 2 capas densas.\n",
    "    # la primera, tendrá 25 neuronas, activación relu e inicialización 'he_uniform' (https://keras.io/initializers/)\n",
    "    # además, tendremos que indicarle las dimensiones de entrada (con el atributo input_dim)\n",
    "    model.add(Dense(N_NEURONAS, activation='ACTIVACIÓN', kernel_initializer='INICIALIZACIÓN_PESOS', input_dim=DIMENSIONES_ENTRADA))\n",
    "    # la segunda, será una densa de 1 única neurona y activación sigmoide: \n",
    "    # queremos que nos de la probabilidad de que la imagen sea real\n",
    "    model.add(Dense(N_NEURONAS, activation='ACTIVACIÓN'))\n",
    "    # utilizamos función de activación final sigmoide y la cross entropía\n",
    "    # binaria para que la red nos devuelva las probabilidades de que la entrada\n",
    "    # sea falsa y real\n",
    "    # el optimizador que usaremos en este caso es el adam con los parámetros por defecto\n",
    "    model.compile(loss='AQUÍ_LA_FUNCIÓN_DE_PÉRDIDAS', optimizer='OPTIMIZADOR', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Qt1L-2ybrma"
   },
   "source": [
    "Perfecto, ya lo tenemos.\n",
    "\n",
    "Ahora necesitamos el generador. ¿Qué es lo que necesitamos que haga el generador? Que genere 2 números que vamos a intentar aproximar a $(x, x^2)$ mediante entrenamiento.\n",
    "\n",
    "Vamos a ello:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uuXmHlE7b7kS"
   },
   "outputs": [],
   "source": [
    "# definimos el generador\n",
    "def define_generator(latent_dim, n_outputs=2):\n",
    "    # fijaos en varios detalles:\n",
    "    # - latent_dim: es el tamaño del vector de ruido de la entrada. \n",
    "    # Cuanto mayor sea, más complejos serán los datos que podremos generar\n",
    "    # - la función de activación final ha de ser lineal: esto es muy importante, \n",
    "    # ya que estamos haciendo un trabajo de regresión (estamos tratando de\n",
    "    # llegar a $(x, x^2)$ desde el valor introducido a la red (el vector latente)\n",
    "    # - no compilamos el modelo: esto se debe a que durante el entrenamiento \n",
    "    # del G(z) vamos a necesitar acoplar G(z) a D(x)\n",
    "\n",
    "    # vamos a definir un modelo haciendo uso del modelo Sequential de keras\n",
    "    model = MODELO\n",
    "    # le añadiremos una capa densa de 15 neuronas, con activación relu e\n",
    "    # inicialización de pesos 'he_uniform', y tendremos que indicarle las \n",
    "    # dimensiones de entrada, igual que en el caso del discriminador\n",
    "    model.add(AQUÍ_TU_CAPA) \n",
    "    # la última capa tendrá tantas neuronas como números esperamos generar\n",
    "    # (en este caso x y x^2) y activación lineal\n",
    "    model.add(AQUÍ_TU_CAPA)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LbRHRFr9dEgH"
   },
   "source": [
    "Generador definido. Ahora vamos a combinar el generador y el discriminador para crear el modelo Generativo Adversarial:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wAt6NsGVdMWX"
   },
   "outputs": [],
   "source": [
    "# definimos el modelo GAN combinando generador y discriminador, para entrenar el generador\n",
    "def define_gan(generator, discriminator):\n",
    "    # recordad que durante el entrenamiento del generador necesitamos al\n",
    "    # discriminador para que nos diga si la muestra es real o no, pero\n",
    "    # no debemos actualizar los pesos del discriminador, ya que si no,\n",
    "    # estaríamos haciendo trampas: estaríamos haciendo peor al D(x)\n",
    "    # Así que congelamos el discriminador:\n",
    "    discriminator.trainable = HACER_QUE_NO_SEA_ENTRENABLE\n",
    "    # ahora conectamos el G(z) al D(x)\n",
    "    model = Sequential()\n",
    "    # añadimos el generador primero: él es el encargado de generar una muestra\n",
    "    # a partir del espacio latente\n",
    "    model.add(MODELO_A_AÑADIR)\n",
    "    # y el discriminador después: le introducimos la muestra generada por el \n",
    "    # G(z) para que nos diga si cree que es real o fake\n",
    "    model.add(MODELO_A_AÑADIR)\n",
    "    # y ahora sí, compilamos el modelo con optimizador adam y función de\n",
    "    # pérdidas binary crossentropy\n",
    "    COMPILAR MODELO CONFORME SE ESPECIFICA\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0kx9ocDfd2eO"
   },
   "source": [
    "Bueno, ya lo tenemos todo, ¿no?\n",
    "\n",
    "Pues casi, pero no: nos faltan un par de funciones, una que nos devuelva el vector latente que introducirle al G(z) y otra que coja ese vector latente y produzca una muestra fake. \n",
    "\n",
    "Fijaos lo sencillo que es:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q4Rjb3TheBbQ"
   },
   "outputs": [],
   "source": [
    "# generamos los vectores latentes que introduciremos al generador\n",
    "def generate_latent_points(latent_dim, batch_size):\n",
    "    # generamos un vector de batch_size * latent_dim números aleatorios\n",
    "    # latent_dim es la dimensión del vector latente\n",
    "    # batch_size es el número de elementos por batch\n",
    "    x_input = randn(latent_dim * batch_size)\n",
    "    # redimensionamos el vector para que tenga un tamaño (batch_size, latent_dim)\n",
    "    x_input = x_input.reshape(batch_size, latent_dim)\n",
    "    return x_input\n",
    "\n",
    "# creamos datos fake con el generador (dinero falsificado)\n",
    "def generate_fake_samples(generator, latent_dim, n): \n",
    "    # usamos la función anterior para generar los vectores latentes que \n",
    "    # necesitamos para generar muestras fake\n",
    "    # queremos generar 'n' códigos latentes de 'latent_dim'\n",
    "    x_input = generate_latent_points(latent_dim, n)\n",
    "    # le introducimos los vectores latentes al generador para obtener\n",
    "    # muestras similares a las reales\n",
    "    X = generator.predict(x_input)\n",
    "    # le asignamos la etiqueta 0 (porque son falsas)\n",
    "    y = np.zeros((n, 1)) \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NIFHKIGWfKks"
   },
   "source": [
    "¡Ahora sí! Ya solo nos falta implementar el bucle de entrenamiento. Vamos a ello:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yx9Fe0AlfOqO"
   },
   "outputs": [],
   "source": [
    "# función para entrenar la GAN: el discriminador y el generador\n",
    "def train(g_model, d_model, gan_model, latent_dim, n_epochs=10000, n_batch=128, n_eval=2000):\n",
    "    # para entrenar el discriminador, le introduciremos la mitad de los datos\n",
    "    # reales y la otra mitad falsa\n",
    "    half_batch = int(n_batch / 2)\n",
    "    for epoch in tqdm(range(n_epochs)):\n",
    "        \n",
    "        # preparamos los datos reales (generados con f(x)=x^2)\n",
    "        x_real, y_real = generate_real_samples(half_batch)\n",
    "        \n",
    "        # preparamos los datos falsos (creados con el generador)\n",
    "        x_fake, y_fake = generate_fake_samples(g_model, latent_dim, half_batch)\n",
    "        \n",
    "        # entrenamos el discriminador\n",
    "        d_model.train_on_batch(x_real, y_real) # primero con los datos reales\n",
    "        d_model.train_on_batch(RELLENAR) # luego con los datos fake\n",
    "        \n",
    "        # preparamos los puntos en el espacio latente: serán la entrada al\n",
    "        # modelo GAN con el que entrenaremos el generador\n",
    "        x_gan = GENERAR_CÓDIGOS_LATENTES\n",
    "        \n",
    "        # creamos etiquetas invertidas para el generador: utilizamos el D(x) \n",
    "        # para que piense que las muestras que le introducimos son reales, y\n",
    "        # en caso de que diga que no son reales, aprovechamos la información\n",
    "        # de sus gradientes para actualizar el G(z) para que la próxima vez\n",
    "        # los datos generados por G(z) sean más plausibles (parecidos a los \n",
    "        # reales)\n",
    "        y_gan = np.ones((n_batch, VALOR_ETIQUETA))\n",
    "        \n",
    "        # como acabamos de ver, entrenamos el generador de forma que actualice\n",
    "        # sus pesos usando los gradientes del discriminador\n",
    "        # tened en cuenta que en este modelo (gan_model) el discriminador está\n",
    "        # congelado, por lo que no se actualizan sus pesos: no queremos \"untar\"\n",
    "        # a nuestro policía, lo que queremos es fabricar dinero más realista.\n",
    "        gan_model.train_on_batch(x_gan, y_gan)\n",
    "\n",
    "        # evaluamos el modelo cada n_eval épocas\n",
    "        if (epoch+1) % n_eval == 0 or epoch == 0:\n",
    "            # preparamos ejemplos reales\n",
    "            x_real, y_real = generate_real_samples(n_batch)\n",
    "            # evaluamos el discriminador con datos reales\n",
    "            _, acc_real = discriminator.evaluate(x_real, y_real, verbose=0)\n",
    "            # preparamos ejemplos fake\n",
    "            x_fake, y_fake = generate_fake_samples(generator, latent_dim, n_batch)\n",
    "            # evaluamos el discriminador con datos fake\n",
    "            _, acc_fake = discriminator.evaluate(x_fake, y_fake, verbose=0)\n",
    "            # mostramos cómo de bueno es nuestro policía\n",
    "            print(epoch, acc_real, acc_fake)\n",
    "            # lo ploteamos\n",
    "            plt.scatter(x_real[:, 0], x_real[:, 1], color='red')\n",
    "            plt.scatter(x_fake[:, 0], x_fake[:, 1], color='blue') \n",
    "            # lo guardamos para tenerlo disponible más tarde\n",
    "            filename = 'generated_plot_e%03d.png' % (epoch+1) \n",
    "            plt.savefig(filename)\n",
    "            plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vQ7top0De_tn"
   },
   "outputs": [],
   "source": [
    "# comenzamos con el entrenamiento\n",
    "\n",
    "# tamaño del espacio latente\n",
    "latent_dim = 5\n",
    "# creamos el discriminador\n",
    "discriminator = CREAR_MODELO_DISCRIMINADOR\n",
    "# creamos el generador\n",
    "generator = CREAR_MODELO_GENERADOR\n",
    "# creamos la GAN\n",
    "gan_model = CREAR_MODELO_GAN\n",
    "# entrenamos!\n",
    "train(generator, discriminator, gan_model, latent_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k-59Xwidifpe"
   },
   "source": [
    "Vamos a ver cómo ha ido el entrenamiento de nuestra red. Para ello, vamos a visualizar la salida del generador al principio, cuando aún no estaba entrenada, y al final, cuando ya estaba entrenada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OxUN_1Jogr1J"
   },
   "outputs": [],
   "source": [
    "ls -lah"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j0H7eRmCGR01"
   },
   "outputs": [],
   "source": [
    "plt.imshow(plt.imread('generated_plot_e0001.png'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "We94v8QvhCPx"
   },
   "outputs": [],
   "source": [
    "plt.imshow(plt.imread('generated_plot_e2000.png'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OdSU3tQ_huoX"
   },
   "outputs": [],
   "source": [
    "plt.imshow(plt.imread('generated_plot_e10000.png'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JH6r217nkec-"
   },
   "source": [
    "¡Fijáos! El generador ha aprendido a generar datos practicamente iguales a los de nuestro dataset original. ¿Os dáis cuenta de la potencia de esto? Y este es el ejemplo más sencillo que he podido poner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bUqEw6gniDn-"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMn5uHBmNGca69ep+os7DDx",
   "collapsed_sections": [],
   "name": "EDEM - GANs - 1. Mi primera GAN.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
