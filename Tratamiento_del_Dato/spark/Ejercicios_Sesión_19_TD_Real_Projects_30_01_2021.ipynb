{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Ejercicios Sesión 19 TD Real Projects 30.01.2021.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "ruHocwYcT4aj"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jq9d0x1OTh2N"
      },
      "source": [
        "# Prerrequisites"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2_DQBVj_KNvL"
      },
      "source": [
        "Installing Spark and Apache Kafka Library in VM\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xEbGSM3_NM-z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6576114c-42d3-485b-df1d-9f1313b34a22"
      },
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://downloads.apache.org/spark/spark-3.0.1/spark-3.0.1-bin-hadoop2.7.tgz\n",
        "!tar xf spark-3.0.1-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark==1.3.0\n",
        "!pip install py4j==0.10.9\n",
        "\n",
        "# For plotting\n",
        "!pip install folium\n",
        "!pip install plotly"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: py4j==0.10.9 in /usr/local/lib/python3.6/dist-packages (0.10.9)\n",
            "Requirement already satisfied: folium in /usr/local/lib/python3.6/dist-packages (0.8.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from folium) (2.23.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from folium) (1.15.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.6/dist-packages (from folium) (2.11.2)\n",
            "Requirement already satisfied: branca>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from folium) (0.4.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from folium) (1.19.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->folium) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->folium) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->folium) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->folium) (2020.12.5)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from jinja2->folium) (1.1.1)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.6/dist-packages (4.4.1)\n",
            "Requirement already satisfied: retrying>=1.3.3 in /usr/local/lib/python3.6/dist-packages (from plotly) (1.3.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from plotly) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gt5sHazLNzqb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a187dd6-fc79-48e5-aedf-d03ac8e6dded"
      },
      "source": [
        "!ls /content"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ngrok\t\t\t      spark-3.0.1-bin-hadoop2.7.tgz\n",
            "ngrok-stable-linux-amd64.zip  spark-3.0.1-bin-hadoop2.7.tgz.1\n",
            "sample_data\t\t      spark-3.0.1-bin-hadoop2.7.tgz.2\n",
            "spark-3.0.1-bin-hadoop2.7\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TP_HtvSAj4sI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a034a00f-3437-46ad-8850-85a3a2915852"
      },
      "source": [
        "!wget -q https://github.com/masfworld/datahack_docker/raw/master/zeppelin/libs/libs-kafka_301.zip --directory-prefix=/content/spark-3.0.1-bin-hadoop2.7/jars/\n",
        "!unzip -n /content/spark-3.0.1-bin-hadoop2.7/jars/libs-kafka_301.zip -d /content/spark-3.0.1-bin-hadoop2.7/jars/\n",
        "!ls /content/spark-3.0.1-bin-hadoop2.7/jars/*kafka*"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  /content/spark-3.0.1-bin-hadoop2.7/jars/libs-kafka_301.zip\n",
            "/content/spark-3.0.1-bin-hadoop2.7/jars/kafka-clients-2.4.1.jar\n",
            "/content/spark-3.0.1-bin-hadoop2.7/jars/libs-kafka_301.zip\n",
            "/content/spark-3.0.1-bin-hadoop2.7/jars/libs-kafka_301.zip.1\n",
            "/content/spark-3.0.1-bin-hadoop2.7/jars/libs-kafka_301.zip.2\n",
            "/content/spark-3.0.1-bin-hadoop2.7/jars/spark-sql-kafka-0-10_2.12-3.0.1.jar\n",
            "/content/spark-3.0.1-bin-hadoop2.7/jars/spark-token-provider-kafka-0-10_2.12-3.0.1.jar\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W0qDLAxMTUYQ"
      },
      "source": [
        "Define the environment (Java & Spark homes)\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nSd4dfANNndH"
      },
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.0.1-bin-hadoop2.7\"\n",
        "os.environ[\"PYSPARK_SUBMIT_ARGS\"] = \"\""
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K_IgZEv2XaDm"
      },
      "source": [
        "Starting Spark Session and print the version\n",
        "\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BDLMbVBATf9K",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "52806ba4-007a-406b-b4f3-db77957fcd9e"
      },
      "source": [
        "import findspark\n",
        "findspark.add_packages([\"org.apache.spark:spark-sql-kafka-0-10_2.12:3.0.1\"])\n",
        "findspark.add_jars([\"/content/spark-3.0.1-bin-hadoop2.7/jars/kafka-clients-2.0.0.jar\",\"/content/spark-3.0.1-bin-hadoop2.7/jars/lz4-java-1.4.1-jar\",\"/content/spark-3.0.1-bin-hadoop2.7/jars/scala-library-2.11.12.jar\",\"/content/spark-3.0.1-bin-hadoop2.7/jars/slf4j-api-1.7.25.jar\",\"/content/spark-3.0.1-bin-hadoop2.7/jars/snappy-java-1.1.7.1.jar\",\"/content/spark-3.0.1-bin-hadoop2.7/jars/spark-sql-kafka-0-10_2.11-2.4.5.jar\",\"/content/spark-3.0.1-bin-hadoop2.7/jars/spark-tags_2.11-2.4.5.jar\",\"/content/spark-3.0.1-bin-hadoop2.7/jars/unused-1.0.0.jar\"])\n",
        "findspark.init(\"spark-3.0.1-bin-hadoop2.7\")# SPARK_HOME\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# create the session\n",
        "spark = SparkSession \\\n",
        "        .builder \\\n",
        "        .master(\"local[*]\") \\\n",
        "        .config(\"spark.ui.port\", \"4050\") \\\n",
        "        .getOrCreate()\n",
        "\n",
        "spark.version"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'3.0.1'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G28MgeRJHKJ5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 223
        },
        "outputId": "c7dcf95a-fd95-4837-e716-4bbb178d583c"
      },
      "source": [
        "spark"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://86c8eef30358:4050\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.0.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>pyspark-shell</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ],
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7f2e7310d470>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qPrOO29YRuDB"
      },
      "source": [
        "# For Pandas conversion optimization\n",
        "spark.conf.set(\"spark.sql.execution.arrow.enabled\", \"true\")"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FNiYuI5dGo8Y"
      },
      "source": [
        "Creating ngrok tunnel to allow Spark UI (Optional)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x4-7fXZiGmqB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "588a334c-d2d3-4aff-8c24-b4a1401a8d56"
      },
      "source": [
        "!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
        "!unzip ngrok-stable-linux-amd64.zip\n",
        "get_ipython().system_raw('./ngrok http 4050 &')\n",
        "!curl -s http://localhost:4040/api/tunnels | python3 -c \\\n",
        "    \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\""
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-01-30 12:13:08--  https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
            "Resolving bin.equinox.io (bin.equinox.io)... 54.196.227.142, 54.145.36.98, 107.21.11.91, ...\n",
            "Connecting to bin.equinox.io (bin.equinox.io)|54.196.227.142|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 13773305 (13M) [application/octet-stream]\n",
            "Saving to: ‘ngrok-stable-linux-amd64.zip.1’\n",
            "\n",
            "ngrok-stable-linux- 100%[===================>]  13.13M  18.4MB/s    in 0.7s    \n",
            "\n",
            "2021-01-30 12:13:09 (18.4 MB/s) - ‘ngrok-stable-linux-amd64.zip.1’ saved [13773305/13773305]\n",
            "\n",
            "Archive:  ngrok-stable-linux-amd64.zip\n",
            "replace ngrok? [y]es, [n]o, [A]ll, [N]one, [r]ename: A\n",
            "  inflating: ngrok                   \n",
            "http://cab39518e2ee.ngrok.io\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ruHocwYcT4aj"
      },
      "source": [
        "# Download Datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "musWXLzBUEQg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "31e3848e-563c-4f8b-8835-3e1ba74a6520"
      },
      "source": [
        "!mkdir -p /dataset\r\n",
        "!wget -q https://github.com/masfworld/datahack_docker/raw/master/zeppelin/data/trades.csv -P /dataset\r\n",
        "!wget -q https://github.com/masfworld/datahack_docker/raw/master/zeppelin/data/trades.json -P /dataset\r\n",
        "!wget -q https://github.com/masfworld/datahack_docker/raw/master/zeppelin/data/offshore_leaks.edges.csv -P /dataset\r\n",
        "!wget -q https://github.com/masfworld/datahack_docker/raw/master/zeppelin/data/offshore_leaks.nodes.address.csv -P /dataset\r\n",
        "!wget -q https://github.com/masfworld/datahack_docker/raw/master/zeppelin/data/offshore_leaks.nodes.intermediary.csv -P /dataset\r\n",
        "!wget -q https://github.com/masfworld/datahack_docker/raw/master/zeppelin/data/offshore_leaks.nodes.officer.csv -P /dataset\r\n",
        "!wget -q https://github.com/masfworld/datahack_docker/raw/master/zeppelin/data/offshore_leaks.nodes.entity.csv -P /dataset\r\n",
        "!ls /dataset"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "offshore_leaks.edges.csv\t       offshore_leaks.nodes.intermediary.csv.1\n",
            "offshore_leaks.edges.csv.1\t       offshore_leaks.nodes.officer.csv\n",
            "offshore_leaks.nodes.address.csv       offshore_leaks.nodes.officer.csv.1\n",
            "offshore_leaks.nodes.address.csv.1     trades.csv\n",
            "offshore_leaks.nodes.entity.csv        trades.csv.1\n",
            "offshore_leaks.nodes.entity.csv.1      trades.json\n",
            "offshore_leaks.nodes.intermediary.csv  trades.json.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "42PI1onm9kIh"
      },
      "source": [
        "# Project 1 - Regulatory Banking Project\r\n",
        "---\r\n",
        "\r\n",
        "Input files: /dataset/trades.csv & /dataset/trades.json\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y-lkbhjumamG"
      },
      "source": [
        "Para cada Trade, calcular su valor de ganancia. Porcentaje de ganancia/perdida del cierre respecto de la apertura"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ebjNK66MgA_i"
      },
      "source": [
        "df1 = spark.read.format(\"csv\") \\\r\n",
        "                .option(\"inferSchema\", \"true\") \\\r\n",
        "                .option(\"header\", \"true\") \\\r\n",
        "                .load(\"/dataset/trades.csv\")"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MoMKD3kZnUc_"
      },
      "source": [
        "def calc_percentage(open_value, close_value):\r\n",
        "  return ((close_value-open_value)/open_value)*100"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FM0J9ixhojLz",
        "outputId": "2cf4d9cb-27e9-4707-e902-b41bb675aceb"
      },
      "source": [
        "from pyspark.sql.functions import *\r\n",
        "\r\n",
        "spark.udf.register(\"udf_calc_percentage\", calc_percentage) "
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<function __main__.calc_percentage>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FHks9WcPnoUT"
      },
      "source": [
        "df1_trade = df1.withColumn(\"pnl\",round(calc_percentage(col(\"Open\"), col(\"Close\")),2)) \\\r\n",
        "                .select(\"Date\", \"Open\", \"Close\", \"pnl\") \\\r\n",
        "                .withColumn(\"source\", lit(\"trade_1\"))"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uyb5QvybxhYg",
        "outputId": "871ffb30-37be-4e2b-ef94-9aa791fcaf6b"
      },
      "source": [
        "df1_trade.show()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----------+-------+------+-----+-------+\n",
            "|      Date|   Open| Close|  pnl| source|\n",
            "+----------+-------+------+-----+-------+\n",
            "|2017-12-28| 190.91|189.78|-0.59|trade_1|\n",
            "|2017-12-27|  190.6|190.19|-0.22|trade_1|\n",
            "|2017-12-26| 188.53|190.36| 0.97|trade_1|\n",
            "|2017-12-22|  188.2|188.13|-0.04|trade_1|\n",
            "|2017-12-21|  187.7|188.08|  0.2|trade_1|\n",
            "|2017-12-20| 187.14|187.31| 0.09|trade_1|\n",
            "|2017-12-19|185.235|185.98|  0.4|trade_1|\n",
            "|2017-12-18| 183.96|184.73| 0.42|trade_1|\n",
            "|2017-12-15|183.005|182.58|-0.23|trade_1|\n",
            "|2017-12-14| 183.88|182.13|-0.95|trade_1|\n",
            "|2017-12-13| 182.01|183.03| 0.56|trade_1|\n",
            "|2017-12-12| 182.75| 181.8|-0.52|trade_1|\n",
            "|2017-12-11|  182.9|182.25|-0.36|trade_1|\n",
            "|2017-12-08|  182.5|183.41|  0.5|trade_1|\n",
            "|2017-12-07| 180.05| 182.0| 1.08|trade_1|\n",
            "|2017-12-06| 180.25| 180.8| 0.31|trade_1|\n",
            "|2017-12-05| 184.79|182.85|-1.05|trade_1|\n",
            "|2017-12-04| 183.19| 184.9| 0.93|trade_1|\n",
            "|2017-12-01| 180.32|180.42| 0.06|trade_1|\n",
            "|2017-11-30| 178.07|179.82| 0.98|trade_1|\n",
            "+----------+-------+------+-----+-------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1h6LICd9ZBPw"
      },
      "source": [
        "# Project 2 - Transactions Notifications\r\n",
        "\r\n",
        "*Hint: https://databricks.com/blog/2017/05/08/event-time-aggregation-watermarking-apache-sparks-structured-streaming.html*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "49tIaJ7gZN2C"
      },
      "source": [
        "from pyspark.sql.functions import from_json, col\r\n",
        "from pyspark.sql.types import StructType, StructField, StringType\r\n",
        "\r\n",
        "df = spark \\\r\n",
        "  .readStream \\\r\n",
        "  .format(\"kafka\") \\\r\n",
        "  .option(\"kafka.bootstrap.servers\", \"ec2-34-232-50-77.compute-1.amazonaws.com:9092\") \\\r\n",
        "  .option(\"subscribe\", \"transactions\") \\\r\n",
        "  .load()"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z6SLWzYRZbud",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "37dbaa60-45f3-4609-b5e9-89b9378c0352"
      },
      "source": [
        "schema = StructType(\r\n",
        "    [\r\n",
        "     StructField('Account No', StringType(), True),\r\n",
        "     StructField('DATE', StringType(), True),\r\n",
        "     StructField('TRANSACTION DETAILS', StringType(), True),\r\n",
        "     StructField('CHQ.NO.', StringType(), True),\r\n",
        "     StructField('VALUE DATE', StringType(), True),\r\n",
        "     StructField(' WITHDRAWAL AMT ', StringType(), True),\r\n",
        "     StructField(' DEPOSIT AMT ', StringType(), True),\r\n",
        "     StructField('BALANCE AMT', StringType(), True)\r\n",
        "    ]\r\n",
        ")\r\n",
        "df.printSchema()\r\n",
        "\r\n",
        "dataset = df.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\", \"timestamp\") \\\r\n",
        "    .withColumn(\"value\", from_json(\"value\", schema)) \\\r\n",
        "    .select(col('key'), col(\"timestamp\"), col('value.*'))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- key: binary (nullable = true)\n",
            " |-- value: binary (nullable = true)\n",
            " |-- topic: string (nullable = true)\n",
            " |-- partition: integer (nullable = true)\n",
            " |-- offset: long (nullable = true)\n",
            " |-- timestamp: timestamp (nullable = true)\n",
            " |-- timestampType: integer (nullable = true)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eh82Gqz7Z1bm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "948c16ac-33a1-42fa-df2f-206b3c5944a4"
      },
      "source": [
        "dataset.writeStream \\\r\n",
        " .outputMode(\"update\") \\\r\n",
        " .format(\"memory\") \\\r\n",
        " .option(\"truncate\", \"false\") \\\r\n",
        " .queryName(\"transactions\") \\\r\n",
        " .start()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.streaming.StreamingQuery at 0x7f2e72889f60>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h2KkvViQaXKj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 721
        },
        "outputId": "d8c274f3-2598-4cda-e4e8-a4eb7f6fb0a9"
      },
      "source": [
        "spark.sql(\"select * from transactions\").show(truncate = False)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-48317f4909e3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"select * from transactions\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/content/spark-3.0.1-bin-hadoop2.7/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    440\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 442\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    444\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mspark-3.0.1-bin-hadoop2.7/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1301\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1303\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
            "\u001b[0;32mspark-3.0.1-bin-hadoop2.7/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1031\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1032\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1033\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1034\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1035\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mspark-3.0.1-bin-hadoop2.7/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m   1198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1199\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1200\u001b[0;31m             \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1201\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Answer received: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1202\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRETURN_MESSAGE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    584\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yl_g8q9PLIm0"
      },
      "source": [
        "Ventana fija de 1 minuto, agrupando por timestamp.\r\n",
        "Numero de transacciones en cada minuto"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Aot7VXNYLPhJ",
        "outputId": "8ec2dcaf-c504-480f-9bfd-95c89e43a4f4"
      },
      "source": [
        "transactions_wd = dataset.groupBy(window(col(\"timestamp\"), \"1 minute\")).count()\r\n",
        "\r\n",
        "transactions_wd.writeStream \\\r\n",
        " .outputMode(\"update\") \\\r\n",
        " .format(\"memory\") \\\r\n",
        " .option(\"truncate\", \"false\") \\\r\n",
        " .queryName(\"transactions_wd\") \\\r\n",
        " .start()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.streaming.StreamingQuery at 0x7f2e72825780>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AW-qjK1ZNxSa",
        "outputId": "e455bc83-dbf8-4efe-9eb2-9dac70e70e93"
      },
      "source": [
        "spark.sql(\"select * from transactions_wd group by window, count order by window desc\").show(truncate = False)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+------------------------------------------+-----+\n",
            "|window                                    |count|\n",
            "+------------------------------------------+-----+\n",
            "|[2021-01-30 12:23:00, 2021-01-30 12:24:00]|500  |\n",
            "|[2021-01-30 12:23:00, 2021-01-30 12:24:00]|400  |\n",
            "|[2021-01-30 12:23:00, 2021-01-30 12:24:00]|200  |\n",
            "|[2021-01-30 12:23:00, 2021-01-30 12:24:00]|300  |\n",
            "|[2021-01-30 12:23:00, 2021-01-30 12:24:00]|100  |\n",
            "|[2021-01-30 12:22:00, 2021-01-30 12:23:00]|250  |\n",
            "|[2021-01-30 12:22:00, 2021-01-30 12:23:00]|600  |\n",
            "|[2021-01-30 12:22:00, 2021-01-30 12:23:00]|350  |\n",
            "|[2021-01-30 12:22:00, 2021-01-30 12:23:00]|100  |\n",
            "|[2021-01-30 12:22:00, 2021-01-30 12:23:00]|450  |\n",
            "|[2021-01-30 12:21:00, 2021-01-30 12:22:00]|300  |\n",
            "|[2021-01-30 12:21:00, 2021-01-30 12:22:00]|600  |\n",
            "|[2021-01-30 12:21:00, 2021-01-30 12:22:00]|400  |\n",
            "|[2021-01-30 12:21:00, 2021-01-30 12:22:00]|50   |\n",
            "|[2021-01-30 12:21:00, 2021-01-30 12:22:00]|150  |\n",
            "|[2021-01-30 12:21:00, 2021-01-30 12:22:00]|500  |\n",
            "|[2021-01-30 12:20:00, 2021-01-30 12:21:00]|322  |\n",
            "|[2021-01-30 12:20:00, 2021-01-30 12:21:00]|200  |\n",
            "|[2021-01-30 12:20:00, 2021-01-30 12:21:00]|50   |\n",
            "|[2021-01-30 12:20:00, 2021-01-30 12:21:00]|450  |\n",
            "+------------------------------------------+-----+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yG8_q4f5Q_E6"
      },
      "source": [
        "# Project 3 - Panama Papers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NApu7049VPwm"
      },
      "source": [
        "Trace \"Spring Song International Co., Ltd.\" entity with Spark SQL using the following dataset</br>\r\n",
        "/dataset/offshore_leaks.nodes.entity.csv </br>\r\n",
        "/dataset/offshore_leaks.nodes.intermediary.csv </br>\r\n",
        "/dataset/offshore_leaks.edges.csv </br>\r\n",
        "/dataset/offshore_leaks.nodes.officer.csv"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ydUZNSrYGyN"
      },
      "source": [
        ""
      ],
      "execution_count": 19,
      "outputs": []
    }
  ]
}