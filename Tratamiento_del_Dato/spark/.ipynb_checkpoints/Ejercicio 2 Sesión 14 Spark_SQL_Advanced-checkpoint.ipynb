{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jq9d0x1OTh2N"
   },
   "source": [
    "# Prerrequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2_DQBVj_KNvL"
   },
   "source": [
    "Installing Spark and Apache Kafka Library in VM\n",
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xEbGSM3_NM-z"
   },
   "outputs": [],
   "source": [
    "# install Java8\n",
    "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
    "# download spark3.0.1\n",
    "!wget -q https://apache.osuosl.org/spark/spark-3.0.1/spark-3.0.1-bin-hadoop3.2.tgz\n",
    "\n",
    "# unzip it\n",
    "!tar xf spark-3.0.1-bin-hadoop3.2.tgz\n",
    "!pip install -q findspark\n",
    "!pip install py4j\n",
    "\n",
    "# For maps\n",
    "!pip install folium\n",
    "!pip install plotly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W0qDLAxMTUYQ"
   },
   "source": [
    "Define the environment (Java & Spark homes)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nSd4dfANNndH"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.0.1-bin-hadoop3.2\"\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"] = \"--master local[*] pyspark-shell\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K_IgZEv2XaDm"
   },
   "source": [
    "Starting Spark Session and print the version\n",
    "\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "BDLMbVBATf9K"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.0.1'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init(\"c:\\src\\spark-3.0.1-bin-hadoop3.2\")# SPARK_HOME\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# create the session\n",
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .master(\"local[*]\") \\\n",
    "        .config(\"spark.ui.port\", \"4500\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "spark.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "G28MgeRJHKJ5"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://DESKTOP-5IEOO84:4501\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1bb71a732c8>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "qPrOO29YRuDB"
   },
   "outputs": [],
   "source": [
    "# For Pandas conversion optimization\n",
    "spark.conf.set(\"spark.sql.execution.arrow.enabled\", \"true\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FNiYuI5dGo8Y"
   },
   "source": [
    "Creating ngrok tunnel to allow Spark UI (Optional)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x4-7fXZiGmqB"
   },
   "outputs": [],
   "source": [
    "!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
    "!unzip -o ngrok-stable-linux-amd64.zip\n",
    "!sleep 2\n",
    "get_ipython().system_raw('./ngrok http 4500 &')\n",
    "!curl -s http://localhost:4040/api/tunnels | python3 -c \\\n",
    "    \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Z0h3dF9Vg4X"
   },
   "source": [
    "# Descargar Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KBDin-0sXgyI"
   },
   "outputs": [],
   "source": [
    "!mkdir -p /dataset\n",
    "!wget -q https://github.com/masfworld/datahack_docker/raw/master/zeppelin/data/bank.csv -P /dataset\n",
    "!wget -q https://github.com/masfworld/datahack_docker/raw/master/zeppelin/data/vehicles.csv -P /dataset\n",
    "!wget -q https://github.com/masfworld/datahack_docker/raw/master/zeppelin/data/characters.csv -P /dataset\n",
    "!wget -q https://github.com/masfworld/datahack_docker/raw/master/zeppelin/data/planets.csv -P /dataset\n",
    "!wget -q https://github.com/masfworld/datahack_docker/raw/master/zeppelin/data/species.csv -P /dataset\n",
    "!ls /dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "02Zwm3NRXS_I"
   },
   "source": [
    "# Windows Partitioning\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h1o6f6QOjTcZ"
   },
   "source": [
    "## Ejemplo 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "USrsDETtstv-"
   },
   "outputs": [],
   "source": [
    "!head /dataset/bank.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yjPpxRyJYA1h"
   },
   "source": [
    "Leyendo Datos del fichero bank.csv a un Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "HnbafeFCVk8d"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "bank_df = spark.read.format(\"csv\") \\\n",
    "  .option(\"sep\", \";\") \\\n",
    "  .option(\"inferSchema\", \"true\") \\\n",
    "  .option(\"header\", \"true\") \\\n",
    "  .load(\"dataset/bank.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "qxhLiwwXk6kD"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------+-------+---------+-------+-------+-------+----+--------+---+-----+--------+--------+-----+--------+--------+---+\n",
      "|age|          job|marital|education|default|balance|housing|loan| contact|day|month|duration|campaign|pdays|previous|poutcome|  y|\n",
      "+---+-------------+-------+---------+-------+-------+-------+----+--------+---+-----+--------+--------+-----+--------+--------+---+\n",
      "| 30|   unemployed|married|  primary|     no|   1787|     no|  no|cellular| 19|  oct|      79|       1|   -1|       0| unknown| no|\n",
      "| 33|     services|married|secondary|     no|   4789|    yes| yes|cellular| 11|  may|     220|       1|  339|       4| failure| no|\n",
      "| 35|   management| single| tertiary|     no|   1350|    yes|  no|cellular| 16|  apr|     185|       1|  330|       1| failure| no|\n",
      "| 30|   management|married| tertiary|     no|   1476|    yes| yes| unknown|  3|  jun|     199|       4|   -1|       0| unknown| no|\n",
      "| 59|  blue-collar|married|secondary|     no|      0|    yes|  no| unknown|  5|  may|     226|       1|   -1|       0| unknown| no|\n",
      "| 35|   management| single| tertiary|     no|    747|     no|  no|cellular| 23|  feb|     141|       2|  176|       3| failure| no|\n",
      "| 36|self-employed|married| tertiary|     no|    307|    yes|  no|cellular| 14|  may|     341|       1|  330|       2|   other| no|\n",
      "| 39|   technician|married|secondary|     no|    147|    yes|  no|cellular|  6|  may|     151|       2|   -1|       0| unknown| no|\n",
      "| 41| entrepreneur|married| tertiary|     no|    221|    yes|  no| unknown| 14|  may|      57|       2|   -1|       0| unknown| no|\n",
      "| 43|     services|married|  primary|     no|    -88|    yes| yes|cellular| 17|  apr|     313|       1|  147|       2| failure| no|\n",
      "| 39|     services|married|secondary|     no|   9374|    yes|  no| unknown| 20|  may|     273|       1|   -1|       0| unknown| no|\n",
      "| 43|       admin.|married|secondary|     no|    264|    yes|  no|cellular| 17|  apr|     113|       2|   -1|       0| unknown| no|\n",
      "| 36|   technician|married| tertiary|     no|   1109|     no|  no|cellular| 13|  aug|     328|       2|   -1|       0| unknown| no|\n",
      "| 20|      student| single|secondary|     no|    502|     no|  no|cellular| 30|  apr|     261|       1|   -1|       0| unknown|yes|\n",
      "| 31|  blue-collar|married|secondary|     no|    360|    yes| yes|cellular| 29|  jan|      89|       1|  241|       1| failure| no|\n",
      "| 40|   management|married| tertiary|     no|    194|     no| yes|cellular| 29|  aug|     189|       2|   -1|       0| unknown| no|\n",
      "| 56|   technician|married|secondary|     no|   4073|     no|  no|cellular| 27|  aug|     239|       5|   -1|       0| unknown| no|\n",
      "| 37|       admin.| single| tertiary|     no|   2317|    yes|  no|cellular| 20|  apr|     114|       1|  152|       2| failure| no|\n",
      "| 25|  blue-collar| single|  primary|     no|   -221|    yes|  no| unknown| 23|  may|     250|       1|   -1|       0| unknown| no|\n",
      "| 31|     services|married|secondary|     no|    132|     no|  no|cellular|  7|  jul|     148|       1|  152|       1|   other| no|\n",
      "+---+-------------+-------+---------+-------+-------+-------+----+--------+---+-----+--------+--------+-----+--------+--------+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bank_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VHDroSQ1ZQSB"
   },
   "source": [
    "**Obtén el balance de las dos personas más jóvenes por tipo de trabajo**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3BEWmhhYqxp8"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "\n",
    "byJob = Window.partitionBy(\"job\").orderBy(\"age\")\n",
    "\n",
    "bank_df \\\n",
    "  .withColumn(\"new_column_job\", row_number().over(byJob)) \\\n",
    "  .filter(col(\"new_column_job\") <= 2) \\\n",
    "  .select(\"age\", \"job\", \"balance\") \\\n",
    "  .orderBy(\"job\", \"age\") \\\n",
    "  .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yzU_4EjAjZgh"
   },
   "source": [
    "## Ejercicio 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sV-NchsMsHvF"
   },
   "source": [
    "**A partir del Dataframe formado a partir del fichero \"bank.csv\". \n",
    "Obtén el Top 3 de máximos balances por estado civil**\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SgireGq6YWEj"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bX0FXU7JawRm"
   },
   "source": [
    "## Ejercicio 2\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eBtPRPnVmpGM"
   },
   "source": [
    "**Carga el fichero de vehicles.csv en un DataFrame**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eYPt59chwJUw"
   },
   "outputs": [],
   "source": [
    "!head /dataset/vehicles.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L6-LoYNzfZFp"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vCDfqm_UxUcn"
   },
   "source": [
    "**Para cada uno de los vehículos, obtén la diferencia de precio (*cost_in_credits*) para cada producto con respecto al más barato en la misma clase de vehículo**\n",
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-IdhtILfwmSF"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8GUV9KLzqR-4"
   },
   "source": [
    "# Joins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tZw41ki6kc3p"
   },
   "source": [
    "## Ejercicio 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YAzx5B5IkVeP"
   },
   "source": [
    "**Crea los dataframes correspondientes para los ficheros \"characters.csv\" y \"planets.csv\". <br/>\n",
    "Obtén la gravedad del planeta para cada personaje. Selecciona sólo el nombre del personaje y planeta además de su gravedad**\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WMlJTJzTkC2j"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HSEBVnVQlU52"
   },
   "source": [
    "## Ejercicio 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JF4fyAaMlXsI"
   },
   "source": [
    "**Revisa el plan de ejecución del ejercicio 3. ¿Qué tipo de join se está ejecutando? ¿Por qué?**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bbYUbaSMlXGc"
   },
   "source": [
    "**Después de revisar el plan de ejecución, ejecuta las siguientes instrucciones**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y8HYt2Eylh4k"
   },
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", '0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SwEnVoVqnj7D"
   },
   "outputs": [],
   "source": [
    "spark.conf.get(\"spark.sql.autoBroadcastJoinThreshold\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pxqgghUnlWe3"
   },
   "source": [
    "**Vuelve a ejecutar la consulta del ejercicio 3 que contiene el Join**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yT-AFIMvm43z"
   },
   "source": [
    "## Ejercicio 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C9UBVU6ym9ue"
   },
   "source": [
    "**Crea un DataFrame a partir del fichero de \"species.csv\" y reparticiona este y el DataFrame de Characters a 100 particiones**\n",
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ohStdyUGnRC0"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tv0AmHi926F7"
   },
   "source": [
    "## Ejercicio 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WR6FPU3b29TP"
   },
   "source": [
    "**Obtén la clasificación de especies para cada personaje. Selecciona sólo el nombre del personaje y su clasificación de especie**<br>\n",
    "Usa los datframes reparticionados\n",
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C0tRj1VIonEX"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xG81SgpY4P3t"
   },
   "source": [
    "## Ejercicio 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "56fGSsmt4adO"
   },
   "source": [
    "**Ejecuta la siguiente operación sobre el DataFrame del ejercicio 6 y observa la diferencia de reparto de rows entre las particiones**\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Io-BtgnHyKZ6"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "Jq9d0x1OTh2N",
    "8Z0h3dF9Vg4X",
    "h1o6f6QOjTcZ",
    "tv0AmHi926F7"
   ],
   "name": "02.Spark_SQL_Advanced_Solution.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
