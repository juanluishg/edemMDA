{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jq9d0x1OTh2N"
   },
   "source": [
    "# Prerrequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2_DQBVj_KNvL"
   },
   "source": [
    "Installing Spark and Apache Kafka Library in VM\n",
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xEbGSM3_NM-z"
   },
   "outputs": [],
   "source": [
    "# install Java8\n",
    "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
    "# download spark3.0.1\n",
    "!wget -q https://apache.osuosl.org/spark/spark-3.0.1/spark-3.0.1-bin-hadoop3.2.tgz\n",
    "\n",
    "# unzip it\n",
    "!tar xf spark-3.0.1-bin-hadoop3.2.tgz\n",
    "!pip install -q findspark\n",
    "!pip install py4j\n",
    "\n",
    "# For maps\n",
    "!pip install folium\n",
    "!pip install plotly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W0qDLAxMTUYQ"
   },
   "source": [
    "Define the environment (Java & Spark homes)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nSd4dfANNndH"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.0.1-bin-hadoop3.2\"\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"] = \"--master local[*] pyspark-shell\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K_IgZEv2XaDm"
   },
   "source": [
    "Starting Spark Session and print the version\n",
    "\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "BDLMbVBATf9K"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.0.1'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init(\"c:\\src\\spark-3.0.1-bin-hadoop3.2\")# SPARK_HOME\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# create the session\n",
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .master(\"local[*]\") \\\n",
    "        .config(\"spark.ui.port\", \"4500\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "spark.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "G28MgeRJHKJ5"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://DESKTOP-5IEOO84:4500\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x277dd7df9c8>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "qPrOO29YRuDB"
   },
   "outputs": [],
   "source": [
    "# For Pandas conversion optimization\n",
    "spark.conf.set(\"spark.sql.execution.arrow.enabled\", \"true\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FNiYuI5dGo8Y"
   },
   "source": [
    "Creating ngrok tunnel to allow Spark UI (Optional)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x4-7fXZiGmqB"
   },
   "outputs": [],
   "source": [
    "!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
    "!unzip -o ngrok-stable-linux-amd64.zip\n",
    "!sleep 2\n",
    "get_ipython().system_raw('./ngrok http 4500 &')\n",
    "!curl -s http://localhost:4040/api/tunnels | python3 -c \\\n",
    "    \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Z0h3dF9Vg4X"
   },
   "source": [
    "# Descargar Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "KBDin-0sXgyI"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\"wget\" no se reconoce como un comando interno o externo,\n",
      "programa o archivo por lotes ejecutable.\n",
      "\"wget\" no se reconoce como un comando interno o externo,\n",
      "programa o archivo por lotes ejecutable.\n",
      "\"wget\" no se reconoce como un comando interno o externo,\n",
      "programa o archivo por lotes ejecutable.\n",
      "\"wget\" no se reconoce como un comando interno o externo,\n",
      "programa o archivo por lotes ejecutable.\n",
      "\"ls\" no se reconoce como un comando interno o externo,\n",
      "programa o archivo por lotes ejecutable.\n"
     ]
    }
   ],
   "source": [
    "#!mkdir -p /dataset\n",
    "!wget -q https://github.com/masfworld/datahack_docker/raw/master/zeppelin/data/bank.csv -P ./dataset\n",
    "!wget -q https://github.com/masfworld/datahack_docker/raw/master/zeppelin/data/vehicles.csv -P ./dataset\n",
    "!wget -q https://github.com/masfworld/datahack_docker/raw/master/zeppelin/data/characters.csv -P ./dataset\n",
    "!wget -q https://github.com/masfworld/datahack_docker/raw/master/zeppelin/data/netflix_titles.csv -P ./dataset\n",
    "!ls ./dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "02Zwm3NRXS_I"
   },
   "source": [
    "# Lectura de Datos con Spark SQL\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h1o6f6QOjTcZ"
   },
   "source": [
    "## Ejemplo 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "USrsDETtstv-"
   },
   "outputs": [],
   "source": [
    "!head /dataset/bank.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yjPpxRyJYA1h"
   },
   "source": [
    "Leyendo Datos desde un RDD, y conviertiéndolo a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HnbafeFCVk8d"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import Row\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "bankText = spark.sparkContext.textFile(\"/dataset/bank.csv\")\n",
    "\n",
    "bank = bankText.map(lambda lineaCsv: lineaCsv.split(\";\"))\\\n",
    ".filter(lambda s: s[0] != \"\\\"age\\\"\") \\\n",
    ".map(lambda row: Row(int(row[0]), row[1].replace(\"\\\"\", \"\"), row[2].replace(\"\\\"\", \"\"), row[3].replace(\"\\\"\", \"\"), row[5].replace(\"\\\"\", \"\"))) \\\n",
    ".toDF([\"age\", \"job\", \"marital\", \"education\", \"balance\"]) \\\n",
    ".withColumn(\"age\", col(\"age\").cast(\"int\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3BEWmhhYqxp8"
   },
   "outputs": [],
   "source": [
    "bank.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I7kQm1A5q00M"
   },
   "outputs": [],
   "source": [
    "bank.registerTempTable(\"bank\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9-JI8uCsroG7"
   },
   "source": [
    "Cargamos una **extensión de Google Colab** para mostrar una tabla con posibilidad de filtro (Debemos devolver un DataFrame en Pandas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KCGzK_zfrm8u"
   },
   "outputs": [],
   "source": [
    "%load_ext google.colab.data_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GE6JMclKbbkw"
   },
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "bank_grouped = bank\\\n",
    ".groupBy(bank.marital) \\\n",
    ".agg({\"balance\": \"avg\"}) \\\n",
    ".select(\"marital\", col(\"avg(balance)\").alias(\"balance_avg\")) \\\n",
    ".orderBy(col(\"balance_avg\").desc())\\\n",
    "\n",
    "bank_grouped.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QQ1vduJ45VtS"
   },
   "outputs": [],
   "source": [
    "bank_grouped.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tTy8ySNSpgpN"
   },
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT marital, avg(balance) as balance_avg FROM bank group by marital\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7CJgXJe-kpnF"
   },
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "fig = px.pie(bank_grouped.toPandas(), values='balance_avg', names='marital', title='By Marital')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yzU_4EjAjZgh"
   },
   "source": [
    "## Ejemplo 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sV-NchsMsHvF"
   },
   "source": [
    "Cargando un fichero CSV como RDD y convirtiendo a DataFrame aplicando un esquema específico usando el método **\"createDataFrame\"**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SgireGq6YWEj"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "bankSchema = StructType([\n",
    "    StructField(\"age\", IntegerType(), False), \n",
    "    StructField(\"job\", StringType(), False),\n",
    "    StructField(\"marital\", StringType(), False),\n",
    "    StructField(\"education\", StringType(), False),\n",
    "    StructField(\"balance\", IntegerType(), False)])\n",
    "\n",
    "bankText = spark.sparkContext.textFile(\"/dataset/bank.csv\")\n",
    "\n",
    "bank = bankText\\\n",
    ".map(lambda s: s.split(\";\")).filter(lambda s: s[0] != \"\\\"age\\\"\")\\\n",
    ".map(lambda s:(int(s[0]), str(s[1]).replace(\"\\\"\", \"\"), str(s[2]).replace(\"\\\"\", \"\"), str(s[3]).replace(\"\\\"\", \"\"), int(s[5]) ))\n",
    "\n",
    "bankdf = spark.createDataFrame(bank, bankSchema)\n",
    "bankdf.registerTempTable(\"bank2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tTSxYSiqtwum"
   },
   "outputs": [],
   "source": [
    "spark.sql(\"select * from bank2 limit 10\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bX0FXU7JawRm"
   },
   "source": [
    "## Ejercicio 1\n",
    "**Carga el fichero \"vehicles.csv\" directamente en un DataFrame, muéstra el contenido por pantalla e imprime el esquema**\n",
    "\n",
    "Apóyate en la siguiente documentación para hacer la lectura directamente en un DataFrame https://spark.apache.org/docs/latest/sql-data-sources-load-save-functions.html\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eYPt59chwJUw"
   },
   "outputs": [],
   "source": [
    "!head /dataset/vehicles.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KhzS2LGscDLc"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vCDfqm_UxUcn"
   },
   "source": [
    "**Filtra el anterior dataframe obteniendo los vehículos cuya capacidad supere 70**\n",
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8GUV9KLzqR-4"
   },
   "source": [
    "# Spark SQL. Funciones de agregación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BbMEyQFOqVnQ"
   },
   "source": [
    "Links útiles:\n",
    "\n",
    "https://spark.apache.org/docs/latest/api/python/pyspark.sql.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tZw41ki6kc3p"
   },
   "source": [
    "## Ejercicio 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YAzx5B5IkVeP"
   },
   "source": [
    "**A partir del dataframe con todos los vehículos cargado en el ejercicio 1, obtén el número de pasarejos medios por clase de vehículo**\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V016awxlb_qm"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HSEBVnVQlU52"
   },
   "source": [
    "## Ejercicio 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JF4fyAaMlXsI"
   },
   "source": [
    "**Carga el fichero \"characters.csv\" y obtén el color de ojos más común entre todos los personajes**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dMBFJW35b-OM"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yT-AFIMvm43z"
   },
   "source": [
    "## Ejercicio 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C9UBVU6ym9ue"
   },
   "source": [
    "**Carga el dataframe completo de personajes en una tabla temporal y obtén mediante SQL el número de personajes por género**\n",
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aLsnA8dcb5jk"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tv0AmHi926F7"
   },
   "source": [
    "## Ejercicio 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WR6FPU3b29TP"
   },
   "source": [
    "**Carga el fichero \"netflix_titles.csv\" en un DataFrame e imprime el esquema**\n",
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5zyuYbll3UrI"
   },
   "outputs": [],
   "source": [
    "!head /dataset/netflix_titles.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "poS3oDQ9b6T0"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xG81SgpY4P3t"
   },
   "source": [
    "## Ejercicio 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "56fGSsmt4adO"
   },
   "source": [
    "**Obtén el año en el que se añadieron más películas (No TV Shows)**\n",
    "**Usa una UDF para obtener el año en el que se añadió al catálogo**\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v5O7kVYDBStC"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "Jq9d0x1OTh2N",
    "8Z0h3dF9Vg4X"
   ],
   "name": "01.Spark_SQL_Solution.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
