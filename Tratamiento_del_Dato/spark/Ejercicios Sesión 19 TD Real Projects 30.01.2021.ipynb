{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "EDEM - Projects",
      "provenance": [],
      "collapsed_sections": [
        "Jq9d0x1OTh2N"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jq9d0x1OTh2N"
      },
      "source": [
        "# Prerrequisites"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2_DQBVj_KNvL"
      },
      "source": [
        "Installing Spark and Apache Kafka Library in VM\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xEbGSM3_NM-z"
      },
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://downloads.apache.org/spark/spark-3.0.1/spark-3.0.1-bin-hadoop2.7.tgz\n",
        "!tar xf spark-3.0.1-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark==1.3.0\n",
        "!pip install py4j==0.10.9\n",
        "\n",
        "# For plotting\n",
        "!pip install folium\n",
        "!pip install plotly"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gt5sHazLNzqb"
      },
      "source": [
        "!ls /content"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TP_HtvSAj4sI"
      },
      "source": [
        "!wget -q https://github.com/masfworld/datahack_docker/raw/master/zeppelin/libs/libs-kafka_301.zip --directory-prefix=/content/spark-3.0.1-bin-hadoop2.7/jars/\n",
        "!unzip -n /content/spark-3.0.1-bin-hadoop2.7/jars/libs-kafka_301.zip -d /content/spark-3.0.1-bin-hadoop2.7/jars/\n",
        "!ls /content/spark-3.0.1-bin-hadoop2.7/jars/*kafka*"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W0qDLAxMTUYQ"
      },
      "source": [
        "Define the environment (Java & Spark homes)\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nSd4dfANNndH"
      },
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.0.1-bin-hadoop2.7\"\n",
        "os.environ[\"PYSPARK_SUBMIT_ARGS\"] = \"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K_IgZEv2XaDm"
      },
      "source": [
        "Starting Spark Session and print the version\n",
        "\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BDLMbVBATf9K"
      },
      "source": [
        "import findspark\n",
        "findspark.add_packages([\"org.apache.spark:spark-sql-kafka-0-10_2.12:3.0.1\"])\n",
        "findspark.add_jars([\"/content/spark-3.0.1-bin-hadoop2.7/jars/kafka-clients-2.0.0.jar\",\"/content/spark-3.0.1-bin-hadoop2.7/jars/lz4-java-1.4.1-jar\",\"/content/spark-3.0.1-bin-hadoop2.7/jars/scala-library-2.11.12.jar\",\"/content/spark-3.0.1-bin-hadoop2.7/jars/slf4j-api-1.7.25.jar\",\"/content/spark-3.0.1-bin-hadoop2.7/jars/snappy-java-1.1.7.1.jar\",\"/content/spark-3.0.1-bin-hadoop2.7/jars/spark-sql-kafka-0-10_2.11-2.4.5.jar\",\"/content/spark-3.0.1-bin-hadoop2.7/jars/spark-tags_2.11-2.4.5.jar\",\"/content/spark-3.0.1-bin-hadoop2.7/jars/unused-1.0.0.jar\"])\n",
        "findspark.init(\"spark-3.0.1-bin-hadoop2.7\")# SPARK_HOME\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# create the session\n",
        "spark = SparkSession \\\n",
        "        .builder \\\n",
        "        .master(\"local[*]\") \\\n",
        "        .config(\"spark.ui.port\", \"4050\") \\\n",
        "        .getOrCreate()\n",
        "\n",
        "spark.version"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G28MgeRJHKJ5"
      },
      "source": [
        "spark"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qPrOO29YRuDB"
      },
      "source": [
        "# For Pandas conversion optimization\n",
        "spark.conf.set(\"spark.sql.execution.arrow.enabled\", \"true\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FNiYuI5dGo8Y"
      },
      "source": [
        "Creating ngrok tunnel to allow Spark UI (Optional)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x4-7fXZiGmqB"
      },
      "source": [
        "!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
        "!unzip ngrok-stable-linux-amd64.zip\n",
        "get_ipython().system_raw('./ngrok http 4050 &')\n",
        "!curl -s http://localhost:4040/api/tunnels | python3 -c \\\n",
        "    \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ruHocwYcT4aj"
      },
      "source": [
        "# Download Datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "musWXLzBUEQg"
      },
      "source": [
        "!mkdir -p /dataset\r\n",
        "!wget -q https://github.com/masfworld/datahack_docker/raw/master/zeppelin/data/trades.csv -P /dataset\r\n",
        "!wget -q https://github.com/masfworld/datahack_docker/raw/master/zeppelin/data/trades.json -P /dataset\r\n",
        "!wget -q https://github.com/masfworld/datahack_docker/raw/master/zeppelin/data/offshore_leaks.edges.csv -P /dataset\r\n",
        "!wget -q https://github.com/masfworld/datahack_docker/raw/master/zeppelin/data/offshore_leaks.nodes.address.csv -P /dataset\r\n",
        "!wget -q https://github.com/masfworld/datahack_docker/raw/master/zeppelin/data/offshore_leaks.nodes.intermediary.csv -P /dataset\r\n",
        "!wget -q https://github.com/masfworld/datahack_docker/raw/master/zeppelin/data/offshore_leaks.nodes.officer.csv -P /dataset\r\n",
        "!wget -q https://github.com/masfworld/datahack_docker/raw/master/zeppelin/data/offshore_leaks.nodes.entity.csv -P /dataset\r\n",
        "!ls /dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "42PI1onm9kIh"
      },
      "source": [
        "# Project 1 - Regulatory Banking Project\r\n",
        "---\r\n",
        "\r\n",
        "Input files: /dataset/trades.csv & /dataset/trades.json\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ebjNK66MgA_i"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1h6LICd9ZBPw"
      },
      "source": [
        "# Project 2 - Transactions Notifications\r\n",
        "\r\n",
        "*Hint: https://databricks.com/blog/2017/05/08/event-time-aggregation-watermarking-apache-sparks-structured-streaming.html*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "49tIaJ7gZN2C"
      },
      "source": [
        "from pyspark.sql.functions import from_json, col\r\n",
        "from pyspark.sql.types import StructType, StructField, StringType\r\n",
        "\r\n",
        "df = spark \\\r\n",
        "  .readStream \\\r\n",
        "  .format(\"kafka\") \\\r\n",
        "  .option(\"kafka.bootstrap.servers\", \"ec2-3-231-22-58.compute-1.amazonaws.com:9092\") \\\r\n",
        "  .option(\"subscribe\", \"transactions\") \\\r\n",
        "  .load()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z6SLWzYRZbud"
      },
      "source": [
        "schema = StructType(\r\n",
        "    [\r\n",
        "     StructField('Account No', StringType(), True),\r\n",
        "     StructField('DATE', StringType(), True),\r\n",
        "     StructField('TRANSACTION DETAILS', StringType(), True),\r\n",
        "     StructField('CHQ.NO.', StringType(), True),\r\n",
        "     StructField('VALUE DATE', StringType(), True),\r\n",
        "     StructField(' WITHDRAWAL AMT ', StringType(), True),\r\n",
        "     StructField(' DEPOSIT AMT ', StringType(), True),\r\n",
        "     StructField('BALANCE AMT', StringType(), True)\r\n",
        "    ]\r\n",
        ")\r\n",
        "df.printSchema()\r\n",
        "\r\n",
        "dataset = df.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\", \"timestamp\") \\\r\n",
        "    .withColumn(\"value\", from_json(\"value\", schema)) \\\r\n",
        "    .select(col('key'), col(\"timestamp\"), col('value.*'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eh82Gqz7Z1bm"
      },
      "source": [
        "dataset_count.writeStream \\\r\n",
        " .outputMode(\"update\") \\\r\n",
        " .format(\"memory\") \\\r\n",
        " .option(\"truncate\", \"false\") \\\r\n",
        " .queryName(\"transactions\") \\\r\n",
        " .start()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h2KkvViQaXKj"
      },
      "source": [
        "spark.sql(\"select * from transactions\").show(truncate = False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yG8_q4f5Q_E6"
      },
      "source": [
        "# Project 3 - Panama Papers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NApu7049VPwm"
      },
      "source": [
        "Trace \"Spring Song International Co., Ltd.\" entity with Spark SQL using the following dataset</br>\r\n",
        "/dataset/offshore_leaks.nodes.entity.csv </br>\r\n",
        "/dataset/offshore_leaks.nodes.intermediary.csv </br>\r\n",
        "/dataset/offshore_leaks.edges.csv </br>\r\n",
        "/dataset/offshore_leaks.nodes.officer.csv"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ydUZNSrYGyN"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}