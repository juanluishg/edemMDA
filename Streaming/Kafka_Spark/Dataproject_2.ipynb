{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ez1mc8B6Yw1Q",
    "outputId": "af093dc9-e370-4a4b-8b45-d7ca961e8a0b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting py4j==0.10.9\n",
      "  Using cached py4j-0.10.9-py2.py3-none-any.whl (198 kB)\n",
      "Installing collected packages: py4j\n",
      "Successfully installed py4j-0.10.9\n",
      "Collecting folium\n",
      "  Using cached folium-0.12.1-py2.py3-none-any.whl (94 kB)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.8/site-packages (from folium) (2.25.1)\n",
      "Requirement already satisfied: jinja2>=2.9 in /opt/conda/lib/python3.8/site-packages (from folium) (2.11.2)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.8/site-packages (from folium) (1.19.5)\n",
      "Collecting branca>=0.3.0\n",
      "  Using cached branca-0.4.2-py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /opt/conda/lib/python3.8/site-packages (from jinja2>=2.9->folium) (1.1.1)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.8/site-packages (from requests->folium) (4.0.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests->folium) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests->folium) (1.26.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests->folium) (2020.12.5)\n",
      "Installing collected packages: branca, folium\n",
      "Successfully installed branca-0.4.2 folium-0.12.1\n",
      "Collecting plotly\n",
      "  Using cached plotly-4.14.3-py2.py3-none-any.whl (13.2 MB)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.8/site-packages (from plotly) (1.15.0)\n",
      "Collecting retrying>=1.3.3\n",
      "  Using cached retrying-1.3.3-py3-none-any.whl\n",
      "Installing collected packages: retrying, plotly\n",
      "Successfully installed plotly-4.14.3 retrying-1.3.3\n",
      "Collecting psycopg2\n",
      "  Using cached psycopg2-2.8.6.tar.gz (383 kB)\n",
      "\u001b[31m    ERROR: Command errored out with exit status 1:\n",
      "     command: /opt/conda/bin/python3.8 -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/tmp/pip-install-8b2_pqqh/psycopg2_f921d333049843af8956aac48bfaaef3/setup.py'\"'\"'; __file__='\"'\"'/tmp/pip-install-8b2_pqqh/psycopg2_f921d333049843af8956aac48bfaaef3/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' egg_info --egg-base /tmp/pip-pip-egg-info-hkkor199\n",
      "         cwd: /tmp/pip-install-8b2_pqqh/psycopg2_f921d333049843af8956aac48bfaaef3/\n",
      "    Complete output (23 lines):\n",
      "    running egg_info\n",
      "    creating /tmp/pip-pip-egg-info-hkkor199/psycopg2.egg-info\n",
      "    writing /tmp/pip-pip-egg-info-hkkor199/psycopg2.egg-info/PKG-INFO\n",
      "    writing dependency_links to /tmp/pip-pip-egg-info-hkkor199/psycopg2.egg-info/dependency_links.txt\n",
      "    writing top-level names to /tmp/pip-pip-egg-info-hkkor199/psycopg2.egg-info/top_level.txt\n",
      "    writing manifest file '/tmp/pip-pip-egg-info-hkkor199/psycopg2.egg-info/SOURCES.txt'\n",
      "    \n",
      "    Error: pg_config executable not found.\n",
      "    \n",
      "    pg_config is required to build psycopg2 from source.  Please add the directory\n",
      "    containing pg_config to the $PATH or specify the full executable path with the\n",
      "    option:\n",
      "    \n",
      "        python setup.py build_ext --pg-config /path/to/pg_config build ...\n",
      "    \n",
      "    or with the pg_config option in 'setup.cfg'.\n",
      "    \n",
      "    If you prefer to avoid building psycopg2 from source, please install the PyPI\n",
      "    'psycopg2-binary' package instead.\n",
      "    \n",
      "    For further information please check the 'doc/src/install.rst' file (also at\n",
      "    <https://www.psycopg.org/docs/install.html>).\n",
      "    \n",
      "    ----------------------------------------\u001b[0m\n",
      "\u001b[31mERROR: Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\u001b[0m\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.8/site-packages (1.1.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.8/site-packages (from pandas) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in /opt/conda/lib/python3.8/site-packages (from pandas) (2020.5)\n",
      "Requirement already satisfied: numpy>=1.15.4 in /opt/conda/lib/python3.8/site-packages (from pandas) (1.19.5)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.8/site-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n",
      "Collecting tweepy\n",
      "  Using cached tweepy-3.10.0-py2.py3-none-any.whl (30 kB)\n",
      "Requirement already satisfied: six>=1.10.0 in /opt/conda/lib/python3.8/site-packages (from tweepy) (1.15.0)\n",
      "Requirement already satisfied: requests[socks]>=2.11.1 in /opt/conda/lib/python3.8/site-packages (from tweepy) (2.25.1)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Using cached requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.8/site-packages (from requests-oauthlib>=0.7.0->tweepy) (3.0.1)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests[socks]>=2.11.1->tweepy) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests[socks]>=2.11.1->tweepy) (1.26.2)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.8/site-packages (from requests[socks]>=2.11.1->tweepy) (4.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests[socks]>=2.11.1->tweepy) (2020.12.5)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /opt/conda/lib/python3.8/site-packages (from requests[socks]>=2.11.1->tweepy) (1.7.1)\n",
      "Installing collected packages: requests-oauthlib, tweepy\n",
      "Successfully installed requests-oauthlib-1.3.0 tweepy-3.10.0\n"
     ]
    }
   ],
   "source": [
    "#!apt-get update\n",
    "#!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
    "#!wget https://downloads.apache.org/spark/spark-3.0.1/spark-3.0.1-bin-hadoop2.7.tgz\n",
    "#!tar xf spark-3.0.1-bin-hadoop2.7.tgz\n",
    "!pip install -q findspark==1.3.0\n",
    "!pip install py4j==0.10.9\n",
    "\n",
    "# For plotting\n",
    "!pip install folium\n",
    "!pip install plotly\n",
    "\n",
    "!pip install psycopg2\n",
    "!pip install pandas\n",
    "!pip install tweepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "pZAypTcIY0AH"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.0.1-bin-hadoop2.7\"\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "tO-Ce5QWY15-",
    "outputId": "f8e86059-0c64-4ff0-acde-b87ba2c8e663"
   },
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (<ipython-input-6-45b1955de6a6>, line 12)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-6-45b1955de6a6>\"\u001b[0;36m, line \u001b[0;32m12\u001b[0m\n\u001b[0;31m    .getOrCreate()\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.add_packages([\"org.apache.spark:spark-sql-kafka-0-10_2.12:3.0.1\"])\n",
    "findspark.add_jars([\"/content/spark-3.0.1-bin-hadoop2.7/jars/kafka-clients-2.0.0.jar\",\"/content/spark-3.0.1-bin-hadoop2.7/jars/lz4-java-1.4.1-jar\",\"/content/spark-3.0.1-bin-hadoop2.7/jars/scala-library-2.11.12.jar\",\"/content/spark-3.0.1-bin-hadoop2.7/jars/slf4j-api-1.7.25.jar\",\"/content/spark-3.0.1-bin-hadoop2.7/jars/snappy-java-1.1.7.1.jar\",\"/content/spark-3.0.1-bin-hadoop2.7/jars/spark-sql-kafka-0-10_2.11-2.4.5.jar\",\"/content/spark-3.0.1-bin-hadoop2.7/jars/spark-tags_2.11-2.4.5.jar\",\"/content/spark-3.0.1-bin-hadoop2.7/jars/unused-1.0.0.jar\"])\n",
    "findspark.init(\"spark-3.0.1-bin-hadoop2.7\")# SPARK_HOME\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .master(\"local[*]\") \\\n",
    "        #.config(\"spark.ui.port\", \"4040\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "spark.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 223
    },
    "id": "C-PzsSkPY6fe",
    "outputId": "b63e6b9c-4726-4583-d30f-492d7f937f31"
   },
   "outputs": [],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-fCca8lkY9sf"
   },
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.execution.arrow.enabled\", \"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VZBYGRQSZxho",
    "outputId": "c87990f6-d906-41c2-e412-8d58742c0a4b"
   },
   "outputs": [],
   "source": [
    "#import psycopg2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import json\n",
    "import tweepy\n",
    "from tweepy import OAuthHandler\n",
    "import requests\n",
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QuKcu1GkZrRt"
   },
   "outputs": [],
   "source": [
    "def connect():\n",
    "    try:\n",
    "        connection = psycopg2.connect(user = \"postgres\",\n",
    "                                      password = \"Welcome01\",\n",
    "                                      host = \"35.238.29.211\",\n",
    "                                      port = \"5432\",\n",
    "                                      database = \"dataproject2\")\n",
    "       \n",
    "        cursor = connection.cursor()\n",
    "        return cursor\n",
    "        \n",
    "    except (Exception, psycopg2.Error) as error :\n",
    "        print (\"Error while connecting to PostgreSQL\", error)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LqWlXMg3Zm1H"
   },
   "outputs": [],
   "source": [
    "def algoritmo(text_person):\n",
    "    tg0 = time.time()\n",
    "    cursor = connect()\n",
    "    \n",
    "    playa = [\"Valencia\", \"Barcelona\", \"Ibiza\"]\n",
    "    ciudad = [\"Madrid\", \"Barcelona\", \"Valencia\", \"Sevilla\"]\n",
    "    naturaleza = [\"Oviedo\", \"Bilbao\"]\n",
    "    fiesta = [\"Ibiza\", \"Madrid\", \"Barcelona\"]\n",
    "    \n",
    "    scoring = {\"Madrid\": 0, \"Barcelona\": 0, \"Ibiza\": 0, \"Valencia\": 0, \"Sevilla\": 0, \"Oviedo\": 0, \"Bilbao\": 0}\n",
    "    \n",
    "    json_person = json.loads(text_person)\n",
    "    tweet_text = json_person['full_text']\n",
    "    tweet_person_id = json_person['id']\n",
    "    \n",
    "    split_text = tweet_text.split(\" \")\n",
    "    try:\n",
    "        name = split_text[3] + \" \" + split_text[4][:-1]\n",
    "        salary = split_text[8][:-1]\n",
    "        members = split_text[19]\n",
    "        beach = split_text[25][-3:-2]\n",
    "        city = split_text[26][-3:-2]\n",
    "        nature = split_text[27][-3:-2]\n",
    "        party = split_text[28][-2:-1]\n",
    "\n",
    "        t0 = time.time()\n",
    "        \n",
    "        for k in scoring:\n",
    "            try:\n",
    "                playa.index(k)\n",
    "                scoring[k] += int(beach)\n",
    "            except (Exception) as error :\n",
    "                print(\"Not present\", error)\n",
    "\n",
    "            try:\n",
    "                ciudad.index(k)\n",
    "                scoring[k] += int(city)\n",
    "            except (Exception) as error :\n",
    "                print(\"Not present\", error)\n",
    "\n",
    "            try:\n",
    "                naturaleza.index(k)\n",
    "                scoring[k] += int(nature)\n",
    "            except (Exception) as error :\n",
    "                print(\"Not present\", error)\n",
    "\n",
    "            try:\n",
    "                fiesta.index(k)\n",
    "                scoring[k] += int(party)\n",
    "            except (Exception) as error :\n",
    "                print(\"Not present\", error)\n",
    "\n",
    "        t1 = time.time()\n",
    "        print(\"Tiempo de scoring: \"+ str(t1-t0))\n",
    "        \n",
    "        t0 = time.time()\n",
    "        cursor.execute(\"SELECT * FROM casas WHERE cost <= \" + str((int(salary)/12)*0.2) + \" and (rooms = \"\n",
    "                       + str(members) + \" or rooms = \" + str(int(members)+1) + \") and c_counter <=4;\")\n",
    "        record = cursor.fetchall()\n",
    "\n",
    "        cursor.execute(\"SELECT COLUMN_NAME FROM INFORMATION_SCHEMA.COLUMNS WHERE TABLE_NAME = N'casas'\")\n",
    "        columns_name = cursor.fetchall()\n",
    "        \n",
    "        t1 = time.time()\n",
    "        print(\"Tiempo de consulta a base de datos: \"+ str(t1-t0))\n",
    "\n",
    "        \"\"\"Convert array of arrays to single array\"\"\"\n",
    "\n",
    "        array_columns_name = np.array(columns_name)\n",
    "        array_columns_name = np.concatenate( array_columns_name, axis=0 )\n",
    "\n",
    "        #print(array_columns_name)\n",
    "\n",
    "        \"\"\"Transform result of query to a pandas dataframe\"\"\"\n",
    "\n",
    "        df = pd.DataFrame(record, columns=array_columns_name)\n",
    "\n",
    "        #casas_coste = df[df.cost <= ((int(salary)/12)*0.2)]\n",
    "\n",
    "        #casas_hab = casas_coste.loc[(casas_coste.rooms == members) | ((casas_coste.rooms.astype(int)) == int(members)+1)]\n",
    "\n",
    "        #casas_libres = casas_hab.loc[casas_hab.c_counter <= 4]\n",
    "        mejor_casa = df\n",
    "        mejor_casa['score'] = mejor_casa['city_name'].map(scoring)\n",
    "        resultado = mejor_casa[mejor_casa.score == max(mejor_casa.score)]\n",
    "\n",
    "        consumer_key = \"ZHwb6JawdsgTUZreb4yZeb2tX\"\n",
    "        consumer_secret = \"2UoU6lEpNeTPM847paK6X6z6BVoUENhPq87rIIWcOycnNhGtpd\"\n",
    "        access_token = \"1346037018132930560-G3afFHkhsPiYlo1yFO3X4mTmdJhTOY\"\n",
    "        access_token_secret = \"jLjtkTK9VQVitacVhLj8QtSdb0mkOto4J9PDDSWR9x9Q0\"\n",
    "\n",
    "        images_url = {\"Madrid\": \"https://www.enforex.com/images/fichas/madrid/ciudad-madrid-2.jpg\", \n",
    "                  \"Barcelona\": \"https://www.alsa.es/documents/21643679/21664598/Barcelona.jpg\", \n",
    "                  \"Ibiza\": \"https://www.iagua.es/sites/default/files/styles/thumbnail-700x700/public/1155x510-ibiza1.jpg?itok=0SafgsGX\", \n",
    "                  \"Valencia\": \"https://static.lasprovincias.es/www/multimedia/202009/30/media/cortadas/valencia-turismo-tarjeta-kfHG-U1203216296067wG-624x385@Las%20Provincias.jpg\", \n",
    "                  \"Sevilla\": \"https://elcorreoweb.es/binrepository/plaza-espana-sevilla_20333351_20200107162131.jpg\", \n",
    "                  \"Oviedo\": \"https://www.iberia.com/ibcomv3/content/landings/OVD.jpg\", \n",
    "                  \"Bilbao\": \"https://www.leonardo-hotels.es/octopus/Upload/images/Pages/bilbao-1920x580.jpg\"}\n",
    "\n",
    "        # Configuración de acceso con las credenciales\n",
    "        auth = OAuthHandler(consumer_key, consumer_secret)\n",
    "        auth.set_access_token(access_token, access_token_secret)\n",
    "\n",
    "        # Listos para hacer la conexión con el API\n",
    "        api = tweepy.API(auth)\n",
    "\n",
    "        t0 = time.time()\n",
    "        contador = 0\n",
    "        for _, i in resultado.iterrows():\n",
    "            # Ofrecer como maximo 2 casas a una misma persona\n",
    "            if contador >= 2:\n",
    "                break;    \n",
    "\n",
    "            tweet = \"Hi! \"+ name + \", your perfect house is placed in \" + i.city_name + \", with the code: \" + str(i.house_id)\n",
    "            tweet += \" https://twitter.com/dlpexercisepro1/status/\" + str(i.tweet_id)\n",
    "\n",
    "            t2 = time.time()\n",
    "            # Download the image\n",
    "            url = images_url.get(i.city_name)\n",
    "            filename = '/tmp/temp.jpg'\n",
    "            request = requests.get(url, stream=True)\n",
    "            if request.status_code == 200:\n",
    "                with open(filename, 'wb') as image:\n",
    "                    for chunk in request:\n",
    "                        image.write(chunk)\n",
    "            else:\n",
    "                return \"Unable to download image\"\n",
    "            \n",
    "            t3 = time.time()\n",
    "            print(\"Tiempo de descarga de imagen: \" +str(t3-t2))\n",
    "\n",
    "            # Publish Tweet with image\n",
    "            api.update_with_media(filename, status=tweet, in_reply_to_status_id = tweet_person_id, auto_populate_reply_metadata = True)\n",
    "\n",
    "            os.remove(filename)   \n",
    "            contador += 1\n",
    "\n",
    "            # Increment house counter\n",
    "            cursor.execute(\"UPDATE casas SET c_counter = c_counter + 1 WHERE tweet_id = \" + str(i.tweet_id))\n",
    "        t1 = time.time()\n",
    "        print(\"Tiempo de publicación: \"+ str(t1-t0))\n",
    "        tg1 = time.time()\n",
    "        print(\"Tiempo total: \"+ str(tg1-tg0))\n",
    "        return \"publicado\"\n",
    "    except (Exception) as error :\n",
    "        return split_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-DEgf4WTZCZY",
    "outputId": "53196c6b-2706-4aed-8c47-80eec2c1e0c8"
   },
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Failed to find data source: kafka. Please deploy the application as per the deployment section of \"Structured Streaming + Kafka Integration Guide\".;",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-8a3154947f9b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtypes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mStructType\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mStructField\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mStringType\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIntegerType\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mreadStream\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"kafka\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark-3.0.1-bin-hadoop2.7/python/pyspark/sql/streaming.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self, path, format, schema, **options)\u001b[0m\n\u001b[1;32m    418\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 420\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mspark-3.0.1-bin-hadoop2.7/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark-3.0.1-bin-hadoop2.7/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    132\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m                 \u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconverted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark-3.0.1-bin-hadoop2.7/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(e)\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: Failed to find data source: kafka. Please deploy the application as per the deployment section of \"Structured Streaming + Kafka Integration Guide\".;"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import from_json, col\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "\n",
    "df = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"broker:29092\") \\\n",
    "    .option(\"subscribe\", \"este\") \\\n",
    "    .load()\n",
    "    \n",
    "#df.printSchema()  \n",
    "\n",
    "schema = StructType(\n",
    "    [\n",
    "     StructField('created_at', StringType(), True),\n",
    "     StructField('id', StringType(), True),\n",
    "     StructField('full_text', StringType(), True),\n",
    "     StructField('text', StringType(), True)\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "#from pyspark.sql.functions import *\n",
    "\n",
    "#spark.udf.register(\"udf_algoritmo\", algoritmo)\n",
    "\n",
    "df1 = df.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\", \"timestamp\") \\\n",
    "        .select(col('key'), col('timestamp'), col('value'))\n",
    "\n",
    "df1.writeStream \\\n",
    "  .outputMode(\"update\") \\\n",
    "  .format(\"memory\") \\\n",
    "  .option(\"truncate\", \"false\") \\\n",
    "  .queryName(\"tweets\") \\\n",
    "  .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rrjaf3wdaBuP",
    "outputId": "edf90c4e-ebfe-48c0-e94d-cbb92bc5124a"
   },
   "outputs": [],
   "source": [
    "spark.sql(\"select * from tweets\").show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Dataproject 2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
