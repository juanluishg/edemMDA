{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UB-UyzRyNBTa"
   },
   "source": [
    "# XGBoost\n",
    "\n",
    "Vamos a ver qué tal funciona el XGBoost con el problema de la predicción de acciones en bolsa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uS4JZy6v7WmK"
   },
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/jbrownlee/Datasets/master/daily-total-female-births.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tv4ja8CprwN-"
   },
   "source": [
    "# 1. Predicción del valor de acciones en bolsa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4bI_XKR7q1FZ"
   },
   "source": [
    "Nos vamos a conectar a la API de Yahoo Stocks para descargarnos los últimos 20 años de datos de la cotización en bolsa de Amazon e intentar predecir valores futuros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SFKdndZf8xkw"
   },
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "from pandas_datareader import data\n",
    "\n",
    "dataset_raw = data.DataReader('AMZN','yahoo', dt.datetime(2000,1,1), dt.datetime.now())\n",
    "dataset_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OPDe4yIshC9N"
   },
   "outputs": [],
   "source": [
    "# Nos quedamos con la variable 'Close' unicamente\n",
    "dataframe = dataset_raw[['Close']]\n",
    "dataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "krWv-W5ehJkL"
   },
   "outputs": [],
   "source": [
    "# utilizamos la función create_dataset que NO introduce T 0's iniciales\n",
    "def create_dataset(dataset, look_back_memory=1):\n",
    "    dataX, dataY = [], []\n",
    "    for i in range(len(dataset)-look_back_memory-1):\n",
    "        dataX.append(dataset[i:i+look_back_memory, 0])\n",
    "        dataY.append(dataset[i+look_back_memory, 0])\n",
    "    return numpy.array(dataX), numpy.array(dataY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MNV-oZ3KhMAS"
   },
   "outputs": [],
   "source": [
    "# hacemos el import de todo lo que utilizaremos\n",
    "import numpy\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas import read_csv\n",
    "import math\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IoaqyZOXgfRC"
   },
   "outputs": [],
   "source": [
    "# fijamos la semilla para obtener resultados reproducibles\n",
    "numpy.random.seed(42)\n",
    "\n",
    "# cargamos los datos\n",
    "dataset = dataframe.values\n",
    "dataset = dataset.astype('float32')\n",
    "\n",
    "# normalizamos el dataset\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "dataset = scaler.fit_transform(dataset)\n",
    "\n",
    "# dividimos en train y test\n",
    "# train_size = int(len(dataset) * 0.67)\n",
    "train_size = int(len(dataset) * 0.67)\n",
    "test_size = len(dataset) - train_size\n",
    "train, test = dataset[0:train_size,:], dataset[train_size:len(dataset),:]\n",
    "\n",
    "# transformamos los datos para crearnos N registros con T timestamps cada uno \n",
    "# (uno por cada instante temporal hasta completar el tamaño de la ventana) y \n",
    "# las V variables de las que disponga nuestro dataset. En este caso, vamos a \n",
    "# escoger una ventana con un único timestamp T=10 y solo tendremos una variable,\n",
    "# con lo que V=1 (valor de cierre de la acción).\n",
    "look_back_memory = 10\n",
    "trainX, trainY = create_dataset(train, look_back_memory)\n",
    "testX, testY = create_dataset(test, look_back_memory)\n",
    "print(trainX.shape, trainY.shape)\n",
    "print(testX.shape, testY.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PxI1MIjihP0O"
   },
   "outputs": [],
   "source": [
    "# # Nos aseguramos de que las dimensiones de las entradas son las correctas:\n",
    "# # (número de ventanas de T elementos, los T elementos de cada ventana, las V variables de cada timestamp)\n",
    "# variables = 1 # (trainX.shape[1])\n",
    "# trainX = numpy.reshape(trainX, (trainX.shape[0], look_back_memory, variables))\n",
    "# testX = numpy.reshape(testX, (testX.shape[0], look_back_memory, variables))\n",
    "# print(trainX.shape)\n",
    "# print(testX.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v1m96vDhf3WE"
   },
   "outputs": [],
   "source": [
    "# entrenamos el modelo con los datos de entrenamiento\n",
    "model = XGBRegressor(objective='reg:squarederror', n_estimators=1000)\n",
    "model.fit(trainX, trainY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "33i_Lt1vhfV3"
   },
   "outputs": [],
   "source": [
    "# vamos a ver qué tal funciona nuestro modelo\n",
    "trainPredict = model.predict(trainX)\n",
    "testPredict = model.predict(testX)\n",
    "\n",
    "# una vez hechas las predicciones, tenemos que des-normalizarlas\n",
    "trainPredict = scaler.inverse_transform([trainPredict])[0]\n",
    "trainY_orig = scaler.inverse_transform([trainY])[0]\n",
    "testPredict = scaler.inverse_transform([testPredict])[0]\n",
    "testY_orig = scaler.inverse_transform([testY])[0]\n",
    "\n",
    "# y ahora calculamos el error cometido en train y en test\n",
    "trainScore = math.sqrt(mean_squared_error(trainY_orig, trainPredict))\n",
    "print('Train Score: %.2f RMSE' % (trainScore))\n",
    "testScore = math.sqrt(mean_squared_error(testY_orig, testPredict))\n",
    "print('Test Score: %.2f RMSE' % (testScore))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LAbEKteUiTME"
   },
   "outputs": [],
   "source": [
    "# por como creamos el dataset de entrenamiento, ahora tenemos que desplazar\n",
    "# nuestras predicciones para que \"cuadren\" con el eje x de los datos originales\n",
    "trainPredictPlot = numpy.empty(dataset.shape[0])\n",
    "trainPredictPlot[:] = numpy.nan\n",
    "trainPredictPlot[look_back_memory:len(trainPredict)+look_back_memory] = trainPredict\n",
    "\n",
    "# y lo mismo para el test\n",
    "testPredictPlot = numpy.empty(dataset.shape[0])\n",
    "testPredictPlot[:] = numpy.nan\n",
    "testPredictPlot[len(trainPredict)+(look_back_memory*2)+1:len(dataset)-1] = testPredict\n",
    "\n",
    "# y mostramos los datos originales, la predicción en training y la predicción en test\n",
    "plt.plot(scaler.inverse_transform(dataset))\n",
    "plt.plot(trainPredictPlot)\n",
    "plt.plot(testPredictPlot)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QSViDzaRgImB"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-0H4WD6SgDzq"
   },
   "source": [
    "### Entrenar usando un Walk-Forward Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1J_qx183gONB"
   },
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "from pandas_datareader import data\n",
    "\n",
    "dataset_raw = data.DataReader('AMZN','yahoo', dt.datetime(2000,1,1), dt.datetime.now())\n",
    "dataset_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xHfwpyUL7N3t"
   },
   "outputs": [],
   "source": [
    "# forecast monthly births with xgboost\n",
    "from numpy import asarray\n",
    "from pandas import read_csv\n",
    "from pandas import DataFrame\n",
    "from pandas import concat\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from xgboost import XGBRegressor\n",
    "from matplotlib import pyplot\n",
    "import numpy\n",
    "from tqdm import tqdm\n",
    "\n",
    "def create_dataset(dataset, look_back_memory=1):\n",
    "    data = []\n",
    "    for i in range(len(dataset)-look_back_memory):\n",
    "        data.append(dataset[i:i+look_back_memory+1, 0])\n",
    "    return numpy.array(data)\n",
    "\n",
    "# split a univariate dataset into train/test sets\n",
    "def train_test_split(data, n_test):\n",
    "\treturn data[:-n_test, :], data[-n_test:, :]\n",
    "\n",
    "# fit an xgboost model and make a one step prediction\n",
    "def xgboost_forecast(train, testX, n_estimators):\n",
    "\t# transform list into array\n",
    "\ttrain = asarray(train)\n",
    "\t# split into input and output columns\n",
    "\ttrainX, trainy = train[:, :-1], train[:, -1]\n",
    "\t# fit model\n",
    "\tmodel = XGBRegressor(objective='reg:squarederror', n_estimators=n_estimators)\n",
    "\tmodel.fit(trainX, trainy)\n",
    "\t# make a one-step prediction\n",
    "\tyhat = model.predict(asarray([testX]))\n",
    "\treturn yhat[0]\n",
    "\n",
    "# walk-forward validation for univariate data\n",
    "def walk_forward_validation(data, n_test, n_estimators=10):\n",
    "\tpredictions = list()\n",
    "\t# split dataset\n",
    "\ttrain, test = train_test_split(data, n_test)\n",
    "\t# seed history with training dataset\n",
    "\thistory = [x for x in train]\n",
    "\t# step over each time-step in the test set\n",
    "\tfor i in tqdm(range(len(test))):\n",
    "\t\t# split test row into input and output columns\n",
    "\t\ttestX, testy = test[i, :-1], test[i, -1]\n",
    "\t\t# fit model on history and make a prediction\n",
    "\t\tyhat = xgboost_forecast(history, testX, n_estimators)\n",
    "\t\t# store forecast in list of predictions\n",
    "\t\tpredictions.append(yhat)\n",
    "\t\t# add actual observation to history for the next loop\n",
    "\t\thistory.append(test[i])\n",
    "\t\t# summarize progress\n",
    "\t\t# print('>expected=%.1f, predicted=%.1f' % (testy, yhat))\n",
    "\t# estimate prediction error\n",
    "\terror = mean_absolute_error(test[:, -1], predictions)\n",
    "\treturn error, test[:, -1], predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K0Xh_R8Zjw1Q"
   },
   "outputs": [],
   "source": [
    "# creamos el dataset como de costumbre\n",
    "data = create_dataset(dataset_raw[['Close']].values, look_back_memory=10)\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-LUxW0E1j4x-"
   },
   "outputs": [],
   "source": [
    "# evaluate\n",
    "mae, y, yhat = walk_forward_validation(data, n_test=1760, n_estimators=10)\n",
    "print('MAE: %.3f' % mae)\n",
    "# plot expected vs preducted\n",
    "pyplot.plot(y, label='Expected')\n",
    "pyplot.plot(yhat, label='Predicted')\n",
    "pyplot.legend()\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZR3HkHxe7kMR"
   },
   "outputs": [],
   "source": [
    "# evaluate\n",
    "mae, y, yhat = walk_forward_validation(data, n_test=1760, n_estimators=100)\n",
    "print('MAE: %.3f' % mae)\n",
    "# plot expected vs preducted\n",
    "pyplot.plot(y, label='Expected')\n",
    "pyplot.plot(yhat, label='Predicted')\n",
    "pyplot.legend()\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MG9NL-uWwVxZ"
   },
   "source": [
    "Como podéis observar, la red es capaz de predecir bastante mejor los valores de cierre cuando utilizamos una ventana más grande.\n",
    "\n",
    "¿Cómo podíarmos mejorar estos resultados?\n",
    "\n",
    "¿Y si utilizamos, además del valor de cierre, el resto de valores?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TGewQdJ-0OkM"
   },
   "source": [
    "## Predicción con más de una variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e56jaGx00T41"
   },
   "source": [
    "En esta ocasión utilizaremos todos los datos que nos brinda la API de Yahoo para predecir el precio de cierre de una acción."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RAdcw__BJdpA"
   },
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "from pandas_datareader import data\n",
    "\n",
    "dataset_raw = data.DataReader('AMZN','yahoo', dt.datetime(2000,1,1), dt.datetime.now())\n",
    "dataset_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oqeabMj1MKUD"
   },
   "outputs": [],
   "source": [
    "dataset_raw.values[0, :-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q7b6HKrJMPe2"
   },
   "outputs": [],
   "source": [
    "dataset_raw.values[1, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9xTr4l_QMeGm"
   },
   "outputs": [],
   "source": [
    "numpy.append(dataset_raw.values[0, :-1], dataset_raw.values[1, -1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EOsavcx5Lk2U"
   },
   "outputs": [],
   "source": [
    "def create_dataset(dataset):\n",
    "    data = []\n",
    "    for i in range(len(dataset)-1):\n",
    "        data.append(numpy.append(dataset_raw.values[i, :-1], dataset_raw.values[i+1, -1]))\n",
    "    return numpy.array(data)\n",
    "\n",
    "data = create_dataset(dataset_raw.values)\n",
    "print(data[:5])\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RY9yce_QJdpO"
   },
   "outputs": [],
   "source": [
    "# forecast monthly births with xgboost\n",
    "from numpy import asarray\n",
    "from pandas import read_csv\n",
    "from pandas import DataFrame\n",
    "from pandas import concat\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from xgboost import XGBRegressor\n",
    "from matplotlib import pyplot\n",
    "import numpy\n",
    "from tqdm import tqdm\n",
    "\n",
    "# split a univariate dataset into train/test sets\n",
    "def train_test_split(data, n_test):\n",
    "\treturn data[:-n_test, :], data[-n_test:, :]\n",
    "\n",
    "# fit an xgboost model and make a one step prediction\n",
    "def xgboost_forecast(train, testX, n_estimators):\n",
    "\t# transform list into array\n",
    "\ttrain = asarray(train)\n",
    "\t# split into input and output columns\n",
    "\ttrainX, trainy = train[:, :-1], train[:, -1]\n",
    "\t# fit model\n",
    "\tmodel = XGBRegressor(objective='reg:squarederror', n_estimators=n_estimators)\n",
    "\tmodel.fit(trainX, trainy)\n",
    "\t# make a one-step prediction\n",
    "\tyhat = model.predict(asarray([testX]))\n",
    "\treturn yhat[0], model\n",
    "\n",
    "# walk-forward validation for univariate data\n",
    "def walk_forward_validation(data, n_test, n_estimators=10):\n",
    "    predictions = list()\n",
    "    models = list()\n",
    "    # split dataset\n",
    "    train, test = train_test_split(data, n_test)\n",
    "    # seed history with training dataset\n",
    "    history = [x for x in train]\n",
    "    # step over each time-step in the test set\n",
    "    for i in tqdm(range(len(test))):\n",
    "        # split test row into input and output columns\n",
    "        testX, testy = test[i, :-1], test[i, -1]\n",
    "        # fit model on history and make a prediction\n",
    "        yhat, model = xgboost_forecast(history, testX, n_estimators)\n",
    "        # store forecast in list of predictions\n",
    "        predictions.append(yhat)\n",
    "        models.append(model)\n",
    "        # add actual observation to history for the next loop\n",
    "        history.append(test[i])\n",
    "        # summarize progress\n",
    "        # print('>expected=%.1f, predicted=%.1f' % (testy, yhat))\n",
    "    # estimate prediction error\n",
    "    error = mean_absolute_error(test[:, -1], predictions)\n",
    "    return error, test[:, -1], predictions, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "07qjkfzXJdpP"
   },
   "outputs": [],
   "source": [
    "# evaluate\n",
    "mae, y, yhat, models = walk_forward_validation(data, n_test=1760, n_estimators=10)\n",
    "print('MAE: %.3f' % mae)\n",
    "# plot expected vs preducted\n",
    "pyplot.plot(y, label='Expected')\n",
    "pyplot.plot(yhat, label='Predicted')\n",
    "pyplot.legend()\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WiY6AUWlQDLZ"
   },
   "source": [
    "Obtenemos un error muy similar a cuando utilizamos solo el valor de cierre (Close). ¿A qué puede deberse esto?\n",
    "\n",
    "Fijaos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BlhAD6lqQKlY"
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.pairplot(dataset_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WzpbJILTRDpF"
   },
   "source": [
    "Como podéis comprobar, las variables están muy correlacionadas entre sí, por lo que es normal que no aporten información al modelo.\n",
    "\n",
    "Además, según con el modelo con el que trabajemos, podría incluso infuir negativamente.\n",
    "\n",
    "Veamos la importancia de las variables obtenida por el XGBoost:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o0wDInmpNMu-"
   },
   "outputs": [],
   "source": [
    "cols = dataset_raw.columns[:-1]\n",
    "imps = numpy.stack([m.feature_importances_ for m in models])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kUIXJXpYN5_V"
   },
   "outputs": [],
   "source": [
    "imps.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZZ9Us9gCOEBw"
   },
   "outputs": [],
   "source": [
    "feature_importances = DataFrame(imps, columns=cols)\n",
    "# feature_importances = feature_importances.sort_values(by='imp',ascending=False)\n",
    "# px.bar(feature_importances,x='col',y='imp')\n",
    "feature_importances.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nQjmvJT1OcTv"
   },
   "source": [
    "Las variables High, Low, Close y Open tienen una importancia muy similar. Viendo las correlaciones, esto nos indica que son redundantes. Por otra parte, Volume no aporta nada.\n",
    "\n",
    "**¿Y si hubiéramos normalizado?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6Gu9y-LLOmRI"
   },
   "outputs": [],
   "source": [
    "def create_dataset(dataset):\n",
    "    data = []\n",
    "    for i in range(len(dataset)-1):\n",
    "        data.append(numpy.append(dataset_raw.values[i, :-1], dataset_raw.values[i+1, -1]))\n",
    "    return numpy.array(data)\n",
    "\n",
    "data = create_dataset(dataset_raw.values)\n",
    "print(data[:5])\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AR39VucTOtyw"
   },
   "outputs": [],
   "source": [
    "data = (data - data.min(axis=0)) / (data.max(axis=0) - data.min(axis=0))\n",
    "data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "53eLpM5OOtn0"
   },
   "outputs": [],
   "source": [
    "data.min(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fhwD52G7Otd5"
   },
   "outputs": [],
   "source": [
    "data.max(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x7O9GrQYOtWR"
   },
   "outputs": [],
   "source": [
    "# forecast monthly births with xgboost\n",
    "from numpy import asarray\n",
    "from pandas import read_csv\n",
    "from pandas import DataFrame\n",
    "from pandas import concat\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from xgboost import XGBRegressor\n",
    "from matplotlib import pyplot\n",
    "import numpy\n",
    "from tqdm import tqdm\n",
    "\n",
    "# split a univariate dataset into train/test sets\n",
    "def train_test_split(data, n_test):\n",
    "\treturn data[:-n_test, :], data[-n_test:, :]\n",
    "\n",
    "# fit an xgboost model and make a one step prediction\n",
    "def xgboost_forecast(train, testX, n_estimators):\n",
    "\t# transform list into array\n",
    "\ttrain = asarray(train)\n",
    "\t# split into input and output columns\n",
    "\ttrainX, trainy = train[:, :-1], train[:, -1]\n",
    "\t# fit model\n",
    "\tmodel = XGBRegressor(objective='reg:squarederror', n_estimators=n_estimators)\n",
    "\tmodel.fit(trainX, trainy)\n",
    "\t# make a one-step prediction\n",
    "\tyhat = model.predict(asarray([testX]))\n",
    "\treturn yhat[0], model\n",
    "\n",
    "# walk-forward validation for univariate data\n",
    "def walk_forward_validation(data, n_test, n_estimators=10):\n",
    "    predictions = list()\n",
    "    models = list()\n",
    "    # split dataset\n",
    "    train, test = train_test_split(data, n_test)\n",
    "    # seed history with training dataset\n",
    "    history = [x for x in train]\n",
    "    # step over each time-step in the test set\n",
    "    for i in tqdm(range(len(test))):\n",
    "        # split test row into input and output columns\n",
    "        testX, testy = test[i, :-1], test[i, -1]\n",
    "        # fit model on history and make a prediction\n",
    "        yhat, model = xgboost_forecast(history, testX, n_estimators)\n",
    "        # store forecast in list of predictions\n",
    "        predictions.append(yhat)\n",
    "        models.append(model)\n",
    "        # add actual observation to history for the next loop\n",
    "        history.append(test[i])\n",
    "        # summarize progress\n",
    "        # print('>expected=%.1f, predicted=%.1f' % (testy, yhat))\n",
    "    # estimate prediction error\n",
    "    error = mean_absolute_error(test[:, -1], predictions)\n",
    "    return error, test[:, -1], predictions, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "erBg4gXNOtMU"
   },
   "outputs": [],
   "source": [
    "# evaluate\n",
    "mae, y, yhat, models = walk_forward_validation(data, n_test=1760, n_estimators=10)\n",
    "print('MAE: %.3f' % mae)\n",
    "\n",
    "# plot expected vs preducted\n",
    "pyplot.plot(y, label='Expected')\n",
    "pyplot.plot(yhat, label='Predicted')\n",
    "pyplot.legend()\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DYlTc6KLPQ4U"
   },
   "outputs": [],
   "source": [
    "cols = dataset_raw.columns[:-1]\n",
    "imps = numpy.stack([m.feature_importances_ for m in models])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oAv69_8oPQ4W"
   },
   "outputs": [],
   "source": [
    "feature_importances = DataFrame(imps, columns=cols)\n",
    "feature_importances.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F9NscEf1Pvj8"
   },
   "source": [
    "Fijaos que el ajuste es mejor, pero sigue sin tener en cuenta la variable Volume para nada."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mwSzSd9j1eBt"
   },
   "source": [
    "**Más ejemplos interesantes de predicción con XGBoost**\n",
    "\n",
    "- Predicción de compras\n",
    "\n",
    "https://www.kaggle.com/alessandrosolbiati/using-xgboost-for-time-series-prediction-top-20\n",
    "\n",
    "- Nacimientos de niñas\n",
    "\n",
    "https://machinelearningmastery.com/xgboost-for-time-series-forecasting/\n",
    "\n",
    "- Consumo de energía eléctrica\n",
    "\n",
    "https://www.kaggle.com/robikscube/tutorial-time-series-forecasting-with-xgboost\n",
    "\n",
    "https://github.com/Jenniferz28/Time-Series-ARIMA-XGBOOST-RNN\n",
    "\n",
    "- Predicción de la demanda de bicicletas:\n",
    "\n",
    "https://towardsdatascience.com/go-highly-accurate-or-go-home-61828afb0b13\n",
    "\n",
    "**Más datasets** \n",
    "\n",
    "https://archive.ics.uci.edu/ml/index.php\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QjOz7VKQ1qJo"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "authorship_tag": "ABX9TyN3/CxdT14CZKy760N9ISfj",
   "collapsed_sections": [],
   "name": "3 - Predicción valores bolsa con XGBoost.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
