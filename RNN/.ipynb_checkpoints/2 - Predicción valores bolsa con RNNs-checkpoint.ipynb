{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UB-UyzRyNBTa"
   },
   "source": [
    "# Redes Neuronales Recurrentes\n",
    "\n",
    "A lo largo de este notebook de prácticas haremos varios ejemplos, tanto con RNNs básicas como con LSTMs, y veremos las diferencias.\n",
    "\n",
    "También trabajaremos con problemas con una única variable y con problemas multivariantes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tv4ja8CprwN-"
   },
   "source": [
    "# 2. Predicción del valor de acciones en bolsa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4bI_XKR7q1FZ"
   },
   "source": [
    "A continuación vamos a ver un ejemplo más real y complejo, que además nos va a servir para introducir los problemas multivariantes.\n",
    "\n",
    "Nos vamos a conectar a la API de Yahoo Stocks para descargarnos los últimos 20 años de datos de la cotización en bolsa de Amazon e intentar predecir valores futuros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas-datareader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "aF4Z0gXmcAl6"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas_datareader'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-918ab21136e5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdatetime\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mdt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mpandas_datareader\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mdataset_raw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'AMZN'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'yahoo'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2000\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mdataset_raw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pandas_datareader'"
     ]
    }
   ],
   "source": [
    "import datetime as dt\n",
    "from pandas_datareader import data\n",
    "\n",
    "dataset_raw = data.DataReader('AMZN','yahoo', dt.datetime(2000,1,1), dt.datetime.now())\n",
    "dataset_raw.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bA1J51btrPFI"
   },
   "source": [
    "### **Ejercicio 4**\n",
    "\n",
    "Trata de predecir el valor de cierre del precio de la acción de Amazon con la mayor exactitud posible. Para ello, puedes emplear tanto RNN como LSTM, elegir el número de celdas, el número de neuronas por celda, y el tamaño de ventana que consideres oportuno. También puedes elegir si quieres rellenar con 0's o no.\n",
    "\n",
    "**NOTA**: Si decides emplear varias celdas, tienes que devolver las \"secuencias\" en las celdas que conectan con subsiguientes celdas. Aquí tienes un ejemplo:\n",
    "\n",
    "```\n",
    "model = Sequential()\n",
    "model.add(SimpleRNN(64, input_shape=(look_back, variables), return_sequences=True)) # celda 1, 64 neuronas\n",
    "model.add(SimpleRNN(32, return_sequences=True)) # celda 2, 32 neuronas\n",
    "model.add(SimpleRNN(16)) # celda 3, 16 neuronas\n",
    "model.add(Dense(8,activation='tanh')) # capa densa con 8 neuronas y activación tanh\n",
    "model.add(Dense(1,activation='linear')) # capa de salida con 1 neurona y activación lineal\n",
    "```\n",
    "\n",
    "Para quedarte con la variable correspondiente al valor de cierre de la acción, puedes hacer lo siguiente:\n",
    "\n",
    "```\n",
    "dataframe = dataset_raw[['Close']]\n",
    "```\n",
    "\n",
    "**Es importante tener en cuenta que no por tener más capas va a funcionar mejor, ya que muy posiblemente ocasionemos overfitting.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TGewQdJ-0OkM"
   },
   "source": [
    "## Predicción con más de una variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e56jaGx00T41"
   },
   "source": [
    "En esta ocasión utilizaremos todos los datos que nos brinda la API de Yahoo para predecir el precio de cierre de una acción."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wDXtTXLIx05J"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import datetime as dt\n",
    "from pandas_datareader import data\n",
    "\n",
    "dataset_raw = data.DataReader('AMZN','yahoo', dt.datetime(2000,1,1), dt.datetime.now())\n",
    "dataset_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "33qZ4naDzoUT"
   },
   "outputs": [],
   "source": [
    "# ligeramente modificada para escoger la variable objetivo\n",
    "def create_dataset(dataset, look_back_memory=1, idx_target=-1):\n",
    "    dataX, dataY = [], []\n",
    "    for i in range(len(dataset)-look_back_memory-1):\n",
    "        dataX.append(dataset[i:i+look_back_memory])\n",
    "        dataY.append(dataset[i+look_back_memory, idx_target])\n",
    "    return np.array(dataX), np.array(dataY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6ZoEYxUx0dkq"
   },
   "source": [
    "Si quisieramos utilizar solo 1 timestamp y nuestra variable objetivo fuese 'Close' (idx=3):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "93ccUZQFzrVi"
   },
   "outputs": [],
   "source": [
    "X, Y = create_dataset(dataset_raw.values, look_back_memory=1, idx_target=3)\n",
    "print(X.shape)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rVFI0rOj0g4_"
   },
   "source": [
    "Pero como hemos visto, funciona mejor con una ventana más amplia. Hagámoslo con 10 timestamps y así podremos compararlo luego con el caso de una sola variable y 10 timestamps que acabamos de hacer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8IM3l1ZMz6_q"
   },
   "outputs": [],
   "source": [
    "X, Y = create_dataset(dataset_raw.values, look_back_memory=10, idx_target=3)\n",
    "print(X.shape)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hu-SmP0D0sKP"
   },
   "outputs": [],
   "source": [
    "# hacemos el import de todo lo que utilizaremos\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas import read_csv\n",
    "import math\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# fijamos la semilla para obtener resultados reproducibles\n",
    "numpy.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cv0gRQiU7d4Z"
   },
   "outputs": [],
   "source": [
    "dataset_raw.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hywq7nW78WyO"
   },
   "outputs": [],
   "source": [
    "dataset_raw.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "61CCZ9GP8g1T"
   },
   "outputs": [],
   "source": [
    "dataframe = dataset_raw.drop(columns=['Adj Close'])\n",
    "dataframe.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8TW8mkxH5y01"
   },
   "outputs": [],
   "source": [
    "# normalizamos el dataset\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "dataset = scaler.fit_transform(dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qv4xPuw5746M"
   },
   "outputs": [],
   "source": [
    "dataset.min(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vy9iuM0C76an"
   },
   "outputs": [],
   "source": [
    "dataset.max(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tXcHx_yZ-Kno"
   },
   "source": [
    "A la hora de hacer las predicciones, des-normalizaremos el valor de Close utilizando su mínimo y su máximo (el penúltimo valor de los siguientes vectores):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Yt-XMfBM-EDr"
   },
   "outputs": [],
   "source": [
    "scaler.data_min_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MXGT9Dge-F2S"
   },
   "outputs": [],
   "source": [
    "scaler.data_max_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WQXGccui6MDE"
   },
   "outputs": [],
   "source": [
    "# dividimos en train y test\n",
    "train_size = int(len(dataset) * 0.67)\n",
    "test_size = len(dataset) - train_size\n",
    "train, test = dataset[0:train_size,:], dataset[train_size:len(dataset),:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ux1UtGGH07Ey"
   },
   "outputs": [],
   "source": [
    "# transformamos los datos para crearnos N registros con T timestamps cada uno \n",
    "# (uno por cada instante temporal hasta completar el tamaño de la ventana) y \n",
    "# las V variables de las que disponga nuestro dataset. En este caso, vamos a \n",
    "# escoger una ventana con un único timestamp T=1 y solo tendremos una variable,\n",
    "# con lo que V=10 (número de pasajeros).\n",
    "look_back_memory = 10\n",
    "trainX, trainY = create_dataset(train, look_back_memory)\n",
    "testX, testY = create_dataset(test, look_back_memory)\n",
    "print(trainX.shape, trainY.shape)\n",
    "print(testX.shape, testY.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_VfvYDDD1EBK"
   },
   "outputs": [],
   "source": [
    "# Nos aseguramos de que las dimensiones de las entradas son las correctas:\n",
    "# (número de ventanas de T elementos, los T elementos de cada ventana, las V variables de cada timestamp)\n",
    "variables = 5\n",
    "trainX = np.reshape(trainX, (trainX.shape[0], look_back_memory, variables))\n",
    "testX = np.reshape(testX, (testX.shape[0], look_back_memory, variables))\n",
    "print(trainX.shape)\n",
    "print(testX.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "15qwICI61IUl"
   },
   "outputs": [],
   "source": [
    "# creamos el modelo y lo entrenamos\n",
    "model = Sequential() #initialize model\n",
    "model.add(LSTM(10, input_shape=(look_back_memory, variables)))\n",
    "model.add(Dense(1, activation='linear'))\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "model.fit(trainX, trainY, epochs=20, batch_size=1, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u5hIqwsx5WPt"
   },
   "outputs": [],
   "source": [
    "# vamos a ver qué tal funciona nuestro modelo\n",
    "trainPredict = model.predict(trainX)\n",
    "testPredict = model.predict(testX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hqu-bLxWhMOw"
   },
   "outputs": [],
   "source": [
    "# una vez hechas las predicciones, tenemos que des-normalizarlas\n",
    "xmin = scaler.data_min_[-2]\n",
    "xmax = scaler.data_max_[-2]\n",
    "trainPredict = trainPredict * (xmax - xmin) + xmin\n",
    "trainY_orig = trainY * (xmax - xmin) + xmin\n",
    "testPredict = testPredict * (xmax - xmin) + xmin\n",
    "testY_orig = testY * (xmax - xmin) + xmin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2UHi2dB4wjd7"
   },
   "outputs": [],
   "source": [
    "# y ahora calculamos el error cometido en train y en test\n",
    "trainScore = math.sqrt(mean_squared_error(trainY_orig, trainPredict[:,0]))\n",
    "print('Train Score: %.2f RMSE' % (trainScore))\n",
    "testScore = math.sqrt(mean_squared_error(testY_orig, testPredict[:,0]))\n",
    "print('Test Score: %.2f RMSE' % (testScore))\n",
    "\n",
    "# por como creamos el dataset de entrenamiento, ahora tenemos que desplazar\n",
    "# nuestras predicciones para que \"cuadren\" con el eje x de los datos originales\n",
    "trainPredictPlot = np.full(dataset.shape[0], np.nan)\n",
    "trainPredictPlot[look_back_memory:len(trainPredict)+look_back_memory] = trainPredict[:, 0]\n",
    "\n",
    "# y lo mismo para el test\n",
    "testPredictPlot = np.full(dataset.shape[0], np.nan)\n",
    "testPredictPlot[len(trainPredict)+(look_back_memory*2)+1:len(dataset)-1] = testPredict[:, 0]\n",
    "\n",
    "# y mostramos los datos originales, la predicción en training y la predicción en test\n",
    "plt.plot(dataset_raw['Close'].values)\n",
    "plt.plot(trainPredictPlot)\n",
    "plt.plot(testPredictPlot)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ys4tQ9nLz0Tc"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mwSzSd9j1eBt"
   },
   "source": [
    "**Más ejemplos interesantes de predicción con LSTMs**\n",
    "\n",
    "- Predicción de potencia consumida en un hogar: \n",
    "\n",
    "https://www.analyticsvidhya.com/blog/2020/10/multivariate-multi-step-time-series-forecasting-using-stacked-lstm-sequence-to-sequence-autoencoder-in-tensorflow-2-0-keras/\n",
    "\n",
    "https://databricks.com/blog/2019/09/10/doing-multivariate-time-series-forecasting-with-recurrent-neural-networks.html\n",
    "\n",
    "- Predicción del precio de BitCoin: \n",
    "\n",
    "https://medium.com/@pierre.beaujuge/multivariate-time-series-forecasting-with-a-bidirectional-lstm-building-a-model-geared-to-4f020a160636\n",
    "\n",
    "-  Predicción de la demanda de bicicletas:\n",
    "\n",
    "https://curiousily.com/posts/demand-prediction-with-lstms-using-tensorflow-2-and-keras-in-python/\n",
    "\n",
    "\n",
    "**Más datasets** \n",
    "\n",
    "https://archive.ics.uci.edu/ml/index.php\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QjOz7VKQ1qJo"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "authorship_tag": "ABX9TyOdoTZYwYn+4dguTpruADIh",
   "collapsed_sections": [
    "UB-UyzRyNBTa"
   ],
   "name": "2 - Predicción valores bolsa con RNNs.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
