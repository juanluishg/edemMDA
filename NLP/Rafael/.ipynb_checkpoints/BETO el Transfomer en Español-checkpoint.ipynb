{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J2B6hSmWpQ0m"
   },
   "source": [
    "# IntroducciÃ³n a BERT con **BETO** (el BERT en espaÃ±ol)\n",
    "\n",
    "https://github.com/dccuchile/beto\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WiBt_ffB9k_K"
   },
   "source": [
    "## DESCARGAMOS EL MODELO\n",
    "\n",
    "El modelo estÃ¡ en PyTorch y por tanto es compatible con la librerÃ­a Transformers de HuggingFace ðŸ¤—.\n",
    "Desafortunadamente el modelo no estÃ¡ en el hub de Transformers para cargarlo de forma directa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 65483,
     "status": "ok",
     "timestamp": 1617965459630,
     "user": {
      "displayName": "Rafael Berlanga Llavori",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gij7a_x7hln_mccXdSKtg2BpPtMu66FEoe7m3u32g=s64",
      "userId": "05947012679519155640"
     },
     "user_tz": -120
    },
    "id": "HhAqZLs3lwhW",
    "outputId": "3b507b83-34b8-4a56-85f8-b26d3398411a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/81/91/61d69d58a1af1bd81d9ca9d62c90a6de3ab80d77f27c5df65d9a2c1f5626/transformers-4.5.0-py3-none-any.whl (2.1MB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2.2MB 19.3MB/s \n",
      "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
      "Collecting tokenizers<0.11,>=0.10.1\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ae/04/5b870f26a858552025a62f1649c20d29d2672c02ff3c3fb4c688ca46467a/tokenizers-0.10.2-cp37-cp37m-manylinux2010_x86_64.whl (3.3MB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3.3MB 54.0MB/s \n",
      "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
      "Collecting sacremoses\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/08/cd/342e584ee544d044fb573ae697404ce22ede086c9e87ce5960772084cad0/sacremoses-0.0.44.tar.gz (862kB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 870kB 51.6MB/s \n",
      "\u001b[?25hRequirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
      "Building wheels for collected packages: sacremoses\n",
      "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for sacremoses: filename=sacremoses-0.0.44-cp37-none-any.whl size=886084 sha256=f8814f7a96e64af75c02918ee9eb0d7942914e972d12ff83438b39302e562476\n",
      "  Stored in directory: /root/.cache/pip/wheels/3e/fb/c0/13ab4d63d537658f448366744654323077c4d90069b6512f3c\n",
      "Successfully built sacremoses\n",
      "Installing collected packages: tokenizers, sacremoses, transformers\n",
      "Successfully installed sacremoses-0.0.44 tokenizers-0.10.2 transformers-4.5.0\n",
      "--2021-04-09 10:50:00--  https://users.dcc.uchile.cl/~jperez/beto/cased_2M/pytorch_weights.tar.gz\n",
      "Resolving users.dcc.uchile.cl (users.dcc.uchile.cl)... 192.80.24.4, 200.9.99.211\n",
      "Connecting to users.dcc.uchile.cl (users.dcc.uchile.cl)|192.80.24.4|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 409871727 (391M) [application/x-gzip]\n",
      "Saving to: â€˜pytorch_weights.tar.gzâ€™\n",
      "\n",
      "pytorch_weights.tar 100%[===================>] 390.88M  9.42MB/s    in 51s     \n",
      "\n",
      "2021-04-09 10:50:52 (7.72 MB/s) - â€˜pytorch_weights.tar.gzâ€™ saved [409871727/409871727]\n",
      "\n",
      "--2021-04-09 10:50:52--  https://users.dcc.uchile.cl/~jperez/beto/cased_2M/vocab.txt\n",
      "Resolving users.dcc.uchile.cl (users.dcc.uchile.cl)... 200.9.99.211, 192.80.24.4\n",
      "Connecting to users.dcc.uchile.cl (users.dcc.uchile.cl)|200.9.99.211|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 242120 (236K) [text/plain]\n",
      "Saving to: â€˜vocab.txtâ€™\n",
      "\n",
      "vocab.txt           100%[===================>] 236.45K   476KB/s    in 0.5s    \n",
      "\n",
      "2021-04-09 10:50:53 (476 KB/s) - â€˜vocab.txtâ€™ saved [242120/242120]\n",
      "\n",
      "--2021-04-09 10:50:53--  https://users.dcc.uchile.cl/~jperez/beto/cased_2M/config.json\n",
      "Resolving users.dcc.uchile.cl (users.dcc.uchile.cl)... 200.9.99.211, 192.80.24.4\n",
      "Connecting to users.dcc.uchile.cl (users.dcc.uchile.cl)|200.9.99.211|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 313 [application/json]\n",
      "Saving to: â€˜config.jsonâ€™\n",
      "\n",
      "config.json         100%[===================>]     313  --.-KB/s    in 0s      \n",
      "\n",
      "2021-04-09 10:50:54 (50.6 MB/s) - â€˜config.jsonâ€™ saved [313/313]\n",
      "\n",
      "pytorch/\n",
      "pytorch/pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "# First install the library and download the models from github\n",
    "\n",
    "!pip install transformers\n",
    "!wget https://users.dcc.uchile.cl/~jperez/beto/cased_2M/pytorch_weights.tar.gz \n",
    "!wget https://users.dcc.uchile.cl/~jperez/beto/cased_2M/vocab.txt \n",
    "!wget https://users.dcc.uchile.cl/~jperez/beto/cased_2M/config.json \n",
    "!tar -xzvf pytorch_weights.tar.gz\n",
    "!mv config.json pytorch/.\n",
    "!mv vocab.txt pytorch/."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YiqMA4l97PZi"
   },
   "source": [
    "## CARGAMOS EL MODELO CON LA TAREA \"MASKED LM\"\n",
    "\n",
    "Se enmascaran algunas palabras de una sentencia y BETO devuelve las candidatas segÃºn su probabilidad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s_1ucCCUnXqa"
   },
   "outputs": [],
   "source": [
    "# Importamos los elementos necesarios para trabajar con los modelos\n",
    "\n",
    "import torch\n",
    "from transformers import BertForMaskedLM, BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4490,
     "status": "ok",
     "timestamp": 1617965502869,
     "user": {
      "displayName": "Rafael Berlanga Llavori",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gij7a_x7hln_mccXdSKtg2BpPtMu66FEoe7m3u32g=s64",
      "userId": "05947012679519155640"
     },
     "user_tz": -120
    },
    "id": "gCpOxoXkoGFC",
    "outputId": "a9ecffa7-3b09-461b-e4ce-fe1533b61e03"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at pytorch/ were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForMaskedLM(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(31002, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (cls): BertOnlyMLMHead(\n",
       "    (predictions): BertLMPredictionHead(\n",
       "      (transform): BertPredictionHeadTransform(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (decoder): Linear(in_features=768, out_features=31002, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creamos el \"tokenizador\" y el modelo con los datos de BETO\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"pytorch/\", do_lower_case=False)\n",
    "model = BertForMaskedLM.from_pretrained(\"pytorch/\")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nJ67Q8AWx6lG"
   },
   "outputs": [],
   "source": [
    "##FunciÃ³n para manejar las mÃ¡scaras del modelo\n",
    "\n",
    "def unmask(model, text):\n",
    "  tokens = tokenizer.tokenize(text)\n",
    "  print(tokens)\n",
    "  indexed_tokens = tokenizer.convert_tokens_to_ids(tokens)\n",
    "  tokens_tensor = torch.tensor([indexed_tokens])\n",
    "  \n",
    "  predictions = model(tokens_tensor)[0]\n",
    "\n",
    "  masked_indxs = [i for i in range(len(tokens)) if tokens[i]=='[MASK]']\n",
    "  for i,midx in enumerate(masked_indxs):\n",
    "    idxs = torch.argsort(predictions[0,midx], descending=True)\n",
    "    predicted_token = tokenizer.convert_ids_to_tokens(idxs[:10])\n",
    "    print('MASK',i,':',predicted_token)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 558,
     "status": "ok",
     "timestamp": 1617966575788,
     "user": {
      "displayName": "Rafael Berlanga Llavori",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gij7a_x7hln_mccXdSKtg2BpPtMu66FEoe7m3u32g=s64",
      "userId": "05947012679519155640"
     },
     "user_tz": -120
    },
    "id": "9KXo6-ahoJoM",
    "outputId": "8add730a-10a1-4bcf-ac39-116ff7db9a0b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'Para', '[MASK]', 'los', 'problemas', 'del', '[MASK]', ',', 'el', 'presidente', 'debe', '[MASK]', 'de', 'inmediato', '.', '[SEP]']\n",
      "MASK 0 : ['resolver', 'solucionar', 'tratar', 'arreglar', 'evitar', 'abordar', 'manejar', 'todos', 'atender', 'corregir']\n",
      "MASK 1 : ['presidente', 'gobierno', 'paÃ­s', 'gabinete', 'Presidente', 'partido', 'mundo', 'sistema', 'estado', 'Congreso']\n",
      "MASK 2 : ['actuar', 'renunciar', 'intervenir', 'presentarse', 'llamar', 'responder', 'regresar', 'reunirse', 'hacerlo', 'retirarse']\n"
     ]
    }
   ],
   "source": [
    "# Probamos ahora el modelo con un problema de \"palabras enmascaradas\"\n",
    "\n",
    "text = \"[CLS] Para [MASK] los problemas del [MASK], el presidente debe [MASK] de inmediato. [SEP]\"\n",
    "\n",
    "unmask(model, text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OQqNkKhK7gDz"
   },
   "source": [
    "Para mejorar la predicciÃ³n de mÃ¡scaras en dominios mÃ¡s concretos serÃ­a necesario re-entrenar el modelo con ejemplos adecuados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L4cokW_j7oxz"
   },
   "source": [
    "## CAMBIAMOS LA TAREA, CAMBIAMOS EL \"HEAD\"\n",
    "\n",
    "Para la clasificaciÃ³n, BERT utiliza el token especial [CLS]. TambiÃ©n se podrÃ­a hacer la clasificaciÃ³n directamente con los embeddings de los token de salida.\n",
    "\n",
    "Cuando cargamos el modelo para una tarea concreta, se cambia el \"head\" del transformer por el de la tarea concreta. Es decir se quita la capa con el que se entrenÃ³ (Ãºltima de las mostradas anteriormente), y se aÃ±ade una nueva para la tarea especÃ­fica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tb9CZLSgQphw"
   },
   "outputs": [],
   "source": [
    "## BERT para clasificar sentencias\n",
    "\n",
    "from transformers import BertForSequenceClassification\n",
    "#import torch\n",
    "\n",
    "model2 = BertForSequenceClassification.from_pretrained(\"pytorch/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2023,
     "status": "ok",
     "timestamp": 1617968606771,
     "user": {
      "displayName": "Rafael Berlanga Llavori",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gij7a_x7hln_mccXdSKtg2BpPtMu66FEoe7m3u32g=s64",
      "userId": "05947012679519155640"
     },
     "user_tz": -120
    },
    "id": "XgcqBhy-bvEW",
    "outputId": "8e62a316-0b48-4053-ecf6-542dd8032ace"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SequenceClassifierOutput(loss=tensor(1.1600, grad_fn=<NllLossBackward>), logits=tensor([[ 0.0911, -0.6928]], grad_fn=<AddmmBackward>), hidden_states=None, attentions=None)\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(\"Me siento triste\", return_tensors='pt')  # Batch size 1\n",
    "labels = torch.tensor([1]).unsqueeze(0)  # Batch size 1\n",
    "outputs = model2(**input_ids, labels=labels)\n",
    "\n",
    "outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eT2Tfucg8Lhw"
   },
   "source": [
    "Este clasificador no clasifica nada porque no le hemos dicho quÃ© tiene que clasificar. De nuevo, Re-entrenando el modelo con ejemplos especÃ­ficos lograremos clasificar con buena precisiÃ³n."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eewYqnEa8FgR"
   },
   "source": [
    "## Y OTRA TAREA ... BUSCAR RESPUESTAS A PREGUNTAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XnCs5T7kmlUO"
   },
   "outputs": [],
   "source": [
    "from transformers import BertForQuestionAnswering\n",
    "#import torch\n",
    "\n",
    "model3 = BertForQuestionAnswering.from_pretrained('pytorch/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 518,
     "status": "ok",
     "timestamp": 1617969386407,
     "user": {
      "displayName": "Rafael Berlanga Llavori",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gij7a_x7hln_mccXdSKtg2BpPtMu66FEoe7m3u32g=s64",
      "userId": "05947012679519155640"
     },
     "user_tz": -120
    },
    "id": "L2oXeouoWaf9",
    "outputId": "50acee0e-d94e-454a-9ec0-aeb166409ae0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'El', 'presidente', 'de', 'la', 'repÃºblica', 'es', 'J', '##uan', 'PÃ©rez', ',', 'que', 'fue', 'nombrado', 'en', '1992', '.', '[SEP]']\n",
      "QuestionAnsweringModelOutput(loss=tensor(3.3530, grad_fn=<DivBackward0>), start_logits=tensor([[0.5718, 0.5167, 0.2103, 0.5927, 0.4087, 0.4196, 0.3866, 0.3108, 0.7489,\n",
      "         0.5119, 0.5294, 0.7897, 0.4597, 0.3280, 0.7372, 0.3749, 0.4290, 0.1096,\n",
      "         0.4879, 0.7059, 0.6806, 0.3969, 0.4912, 0.0961, 0.4025, 0.7489]],\n",
      "       grad_fn=<SqueezeBackward1>), end_logits=tensor([[ 0.0274,  0.0849,  0.2804, -0.1664,  0.0802, -0.0723,  0.1448,  0.0878,\n",
      "         -0.2123,  0.0080,  0.0852,  0.0074, -0.1405,  0.0426,  0.0194, -0.0530,\n",
      "          0.0022, -0.0888,  0.2044,  0.1623,  0.1322,  0.1192, -0.0710, -0.2225,\n",
      "         -0.1345, -0.2124]], grad_fn=<SqueezeBackward1>), hidden_states=None, attentions=None)\n"
     ]
    }
   ],
   "source": [
    "question, text = \"Â¿CÃ³mo se llama el presidente?\", \"El presidente de la repÃºblica es Juan PÃ©rez, que fue nombrado en 1992.\"\n",
    "inputs = tokenizer(question, text, return_tensors='pt')\n",
    "\n",
    "#print(input_ids)\n",
    "tokens_answ = tokenizer.convert_ids_to_tokens(tokenizer.encode(text))\n",
    "print(tokens_answ)\n",
    "\n",
    "#Juan PÃ©rez\n",
    "start_positions = torch.tensor([7])\n",
    "end_positions = torch.tensor([9])\n",
    "\n",
    "output = model3(**inputs, start_positions=start_positions, end_positions=end_positions)\n",
    "\n",
    "output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1098,
     "status": "ok",
     "timestamp": 1617969389473,
     "user": {
      "displayName": "Rafael Berlanga Llavori",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gij7a_x7hln_mccXdSKtg2BpPtMu66FEoe7m3u32g=s64",
      "userId": "05947012679519155640"
     },
     "user_tz": -120
    },
    "id": "GX9ChcazcxGV",
    "outputId": "f6416333-d364-4724-e03f-5cfd19acb1cb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(11) tensor(2)\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "# El segmento a extraer para la respuesta serÃ­a\n",
    "\n",
    "start_scores = torch.argmax(output.start_logits)\n",
    "end_scores   = torch.argmax(output.end_logits)\n",
    "\n",
    "print(start_scores, end_scores)\n",
    "print(tokens_answ[start_scores: end_scores + 1]) # vacÃ­o ya que el final es menor que el principio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YuINSTxU8eMD"
   },
   "source": [
    "No da respuesta porque el final estÃ¡ antes que el principio. El modelo de base no ha sido entrenado para esa tarea y por tanto debe ajustarse con ejemplos concretos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rwFrJ9-ye7Xu"
   },
   "source": [
    "**CONCLUSIÃ“N**: Todos estos modelos deben ser entrenados en la tarea concreta (\"fine-tuning\") para obtener los resultados esperados.\n",
    "Para ello se recomienda utilizar una librerÃ­a mÃ¡s simple enfocada a tareas concretas, como \"SimpleTransformers\""
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "BETO el Transfomer en EspaÃ±ol",
   "provenance": [
    {
     "file_id": "1uRwg4UmPgYIqGYY4gW_Nsw9782GFJbPt",
     "timestamp": 1581328769878
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
