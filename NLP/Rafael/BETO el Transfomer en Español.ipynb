{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J2B6hSmWpQ0m"
   },
   "source": [
    "# Introducci√≥n a BERT con **BETO** (el BERT en espa√±ol)\n",
    "\n",
    "https://github.com/dccuchile/beto\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WiBt_ffB9k_K"
   },
   "source": [
    "## DESCARGAMOS EL MODELO\n",
    "\n",
    "El modelo est√° en PyTorch y por tanto es compatible con la librer√≠a Transformers de HuggingFace ü§ó.\n",
    "Desafortunadamente el modelo no est√° en el hub de Transformers para cargarlo de forma directa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 65483,
     "status": "ok",
     "timestamp": 1617965459630,
     "user": {
      "displayName": "Rafael Berlanga Llavori",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gij7a_x7hln_mccXdSKtg2BpPtMu66FEoe7m3u32g=s64",
      "userId": "05947012679519155640"
     },
     "user_tz": -120
    },
    "id": "HhAqZLs3lwhW",
    "outputId": "3b507b83-34b8-4a56-85f8-b26d3398411a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/81/91/61d69d58a1af1bd81d9ca9d62c90a6de3ab80d77f27c5df65d9a2c1f5626/transformers-4.5.0-py3-none-any.whl (2.1MB)\n",
      "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2.2MB 19.3MB/s \n",
      "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
      "Collecting tokenizers<0.11,>=0.10.1\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ae/04/5b870f26a858552025a62f1649c20d29d2672c02ff3c3fb4c688ca46467a/tokenizers-0.10.2-cp37-cp37m-manylinux2010_x86_64.whl (3.3MB)\n",
      "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3.3MB 54.0MB/s \n",
      "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
      "Collecting sacremoses\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/08/cd/342e584ee544d044fb573ae697404ce22ede086c9e87ce5960772084cad0/sacremoses-0.0.44.tar.gz (862kB)\n",
      "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 870kB 51.6MB/s \n",
      "\u001b[?25hRequirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
      "Building wheels for collected packages: sacremoses\n",
      "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for sacremoses: filename=sacremoses-0.0.44-cp37-none-any.whl size=886084 sha256=f8814f7a96e64af75c02918ee9eb0d7942914e972d12ff83438b39302e562476\n",
      "  Stored in directory: /root/.cache/pip/wheels/3e/fb/c0/13ab4d63d537658f448366744654323077c4d90069b6512f3c\n",
      "Successfully built sacremoses\n",
      "Installing collected packages: tokenizers, sacremoses, transformers\n",
      "Successfully installed sacremoses-0.0.44 tokenizers-0.10.2 transformers-4.5.0\n",
      "--2021-04-09 10:50:00--  https://users.dcc.uchile.cl/~jperez/beto/cased_2M/pytorch_weights.tar.gz\n",
      "Resolving users.dcc.uchile.cl (users.dcc.uchile.cl)... 192.80.24.4, 200.9.99.211\n",
      "Connecting to users.dcc.uchile.cl (users.dcc.uchile.cl)|192.80.24.4|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 409871727 (391M) [application/x-gzip]\n",
      "Saving to: ‚Äòpytorch_weights.tar.gz‚Äô\n",
      "\n",
      "pytorch_weights.tar 100%[===================>] 390.88M  9.42MB/s    in 51s     \n",
      "\n",
      "2021-04-09 10:50:52 (7.72 MB/s) - ‚Äòpytorch_weights.tar.gz‚Äô saved [409871727/409871727]\n",
      "\n",
      "--2021-04-09 10:50:52--  https://users.dcc.uchile.cl/~jperez/beto/cased_2M/vocab.txt\n",
      "Resolving users.dcc.uchile.cl (users.dcc.uchile.cl)... 200.9.99.211, 192.80.24.4\n",
      "Connecting to users.dcc.uchile.cl (users.dcc.uchile.cl)|200.9.99.211|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 242120 (236K) [text/plain]\n",
      "Saving to: ‚Äòvocab.txt‚Äô\n",
      "\n",
      "vocab.txt           100%[===================>] 236.45K   476KB/s    in 0.5s    \n",
      "\n",
      "2021-04-09 10:50:53 (476 KB/s) - ‚Äòvocab.txt‚Äô saved [242120/242120]\n",
      "\n",
      "--2021-04-09 10:50:53--  https://users.dcc.uchile.cl/~jperez/beto/cased_2M/config.json\n",
      "Resolving users.dcc.uchile.cl (users.dcc.uchile.cl)... 200.9.99.211, 192.80.24.4\n",
      "Connecting to users.dcc.uchile.cl (users.dcc.uchile.cl)|200.9.99.211|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 313 [application/json]\n",
      "Saving to: ‚Äòconfig.json‚Äô\n",
      "\n",
      "config.json         100%[===================>]     313  --.-KB/s    in 0s      \n",
      "\n",
      "2021-04-09 10:50:54 (50.6 MB/s) - ‚Äòconfig.json‚Äô saved [313/313]\n",
      "\n",
      "pytorch/\n",
      "pytorch/pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "# First install the library and download the models from github\n",
    "\n",
    "!pip install transformers\n",
    "!wget https://users.dcc.uchile.cl/~jperez/beto/cased_2M/pytorch_weights.tar.gz \n",
    "!wget https://users.dcc.uchile.cl/~jperez/beto/cased_2M/vocab.txt \n",
    "!wget https://users.dcc.uchile.cl/~jperez/beto/cased_2M/config.json \n",
    "!tar -xzvf pytorch_weights.tar.gz\n",
    "!mv config.json pytorch/.\n",
    "!mv vocab.txt pytorch/."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YiqMA4l97PZi"
   },
   "source": [
    "## CARGAMOS EL MODELO CON LA TAREA \"MASKED LM\"\n",
    "\n",
    "Se enmascaran algunas palabras de una sentencia y BETO devuelve las candidatas seg√∫n su probabilidad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s_1ucCCUnXqa"
   },
   "outputs": [],
   "source": [
    "# Importamos los elementos necesarios para trabajar con los modelos\n",
    "\n",
    "import torch\n",
    "from transformers import BertForMaskedLM, BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4490,
     "status": "ok",
     "timestamp": 1617965502869,
     "user": {
      "displayName": "Rafael Berlanga Llavori",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gij7a_x7hln_mccXdSKtg2BpPtMu66FEoe7m3u32g=s64",
      "userId": "05947012679519155640"
     },
     "user_tz": -120
    },
    "id": "gCpOxoXkoGFC",
    "outputId": "a9ecffa7-3b09-461b-e4ce-fe1533b61e03"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at pytorch/ were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForMaskedLM(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(31002, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (cls): BertOnlyMLMHead(\n",
       "    (predictions): BertLMPredictionHead(\n",
       "      (transform): BertPredictionHeadTransform(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (decoder): Linear(in_features=768, out_features=31002, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creamos el \"tokenizador\" y el modelo con los datos de BETO\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"pytorch/\", do_lower_case=False)\n",
    "model = BertForMaskedLM.from_pretrained(\"pytorch/\")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nJ67Q8AWx6lG"
   },
   "outputs": [],
   "source": [
    "##Funci√≥n para manejar las m√°scaras del modelo\n",
    "\n",
    "def unmask(model, text):\n",
    "  tokens = tokenizer.tokenize(text)\n",
    "  print(tokens)\n",
    "  indexed_tokens = tokenizer.convert_tokens_to_ids(tokens)\n",
    "  tokens_tensor = torch.tensor([indexed_tokens])\n",
    "  \n",
    "  predictions = model(tokens_tensor)[0]\n",
    "\n",
    "  masked_indxs = [i for i in range(len(tokens)) if tokens[i]=='[MASK]']\n",
    "  for i,midx in enumerate(masked_indxs):\n",
    "    idxs = torch.argsort(predictions[0,midx], descending=True)\n",
    "    predicted_token = tokenizer.convert_ids_to_tokens(idxs[:10])\n",
    "    print('MASK',i,':',predicted_token)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 558,
     "status": "ok",
     "timestamp": 1617966575788,
     "user": {
      "displayName": "Rafael Berlanga Llavori",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gij7a_x7hln_mccXdSKtg2BpPtMu66FEoe7m3u32g=s64",
      "userId": "05947012679519155640"
     },
     "user_tz": -120
    },
    "id": "9KXo6-ahoJoM",
    "outputId": "8add730a-10a1-4bcf-ac39-116ff7db9a0b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'Para', '[MASK]', 'los', 'problemas', 'del', '[MASK]', ',', 'el', 'presidente', 'debe', '[MASK]', 'de', 'inmediato', '.', '[SEP]']\n",
      "MASK 0 : ['resolver', 'solucionar', 'tratar', 'arreglar', 'evitar', 'abordar', 'manejar', 'todos', 'atender', 'corregir']\n",
      "MASK 1 : ['presidente', 'gobierno', 'pa√≠s', 'gabinete', 'Presidente', 'partido', 'mundo', 'sistema', 'estado', 'Congreso']\n",
      "MASK 2 : ['actuar', 'renunciar', 'intervenir', 'presentarse', 'llamar', 'responder', 'regresar', 'reunirse', 'hacerlo', 'retirarse']\n"
     ]
    }
   ],
   "source": [
    "# Probamos ahora el modelo con un problema de \"palabras enmascaradas\"\n",
    "\n",
    "text = \"[CLS] Para [MASK] los problemas del [MASK], el presidente debe [MASK] de inmediato. [SEP]\"\n",
    "\n",
    "unmask(model, text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OQqNkKhK7gDz"
   },
   "source": [
    "Para mejorar la predicci√≥n de m√°scaras en dominios m√°s concretos ser√≠a necesario re-entrenar el modelo con ejemplos adecuados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L4cokW_j7oxz"
   },
   "source": [
    "## CAMBIAMOS LA TAREA, CAMBIAMOS EL \"HEAD\"\n",
    "\n",
    "Para la clasificaci√≥n, BERT utiliza el token especial [CLS]. Tambi√©n se podr√≠a hacer la clasificaci√≥n directamente con los embeddings de los token de salida.\n",
    "\n",
    "Cuando cargamos el modelo para una tarea concreta, se cambia el \"head\" del transformer por el de la tarea concreta. Es decir se quita la capa con el que se entren√≥ (√∫ltima de las mostradas anteriormente), y se a√±ade una nueva para la tarea espec√≠fica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tb9CZLSgQphw"
   },
   "outputs": [],
   "source": [
    "## BERT para clasificar sentencias\n",
    "\n",
    "from transformers import BertForSequenceClassification\n",
    "#import torch\n",
    "\n",
    "model2 = BertForSequenceClassification.from_pretrained(\"pytorch/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2023,
     "status": "ok",
     "timestamp": 1617968606771,
     "user": {
      "displayName": "Rafael Berlanga Llavori",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gij7a_x7hln_mccXdSKtg2BpPtMu66FEoe7m3u32g=s64",
      "userId": "05947012679519155640"
     },
     "user_tz": -120
    },
    "id": "XgcqBhy-bvEW",
    "outputId": "8e62a316-0b48-4053-ecf6-542dd8032ace"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SequenceClassifierOutput(loss=tensor(1.1600, grad_fn=<NllLossBackward>), logits=tensor([[ 0.0911, -0.6928]], grad_fn=<AddmmBackward>), hidden_states=None, attentions=None)\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(\"Me siento triste\", return_tensors='pt')  # Batch size 1\n",
    "labels = torch.tensor([1]).unsqueeze(0)  # Batch size 1\n",
    "outputs = model2(**input_ids, labels=labels)\n",
    "\n",
    "outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eT2Tfucg8Lhw"
   },
   "source": [
    "Este clasificador no clasifica nada porque no le hemos dicho qu√© tiene que clasificar. De nuevo, Re-entrenando el modelo con ejemplos espec√≠ficos lograremos clasificar con buena precisi√≥n."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eewYqnEa8FgR"
   },
   "source": [
    "## Y OTRA TAREA ... BUSCAR RESPUESTAS A PREGUNTAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XnCs5T7kmlUO"
   },
   "outputs": [],
   "source": [
    "from transformers import BertForQuestionAnswering\n",
    "#import torch\n",
    "\n",
    "model3 = BertForQuestionAnswering.from_pretrained('pytorch/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 518,
     "status": "ok",
     "timestamp": 1617969386407,
     "user": {
      "displayName": "Rafael Berlanga Llavori",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gij7a_x7hln_mccXdSKtg2BpPtMu66FEoe7m3u32g=s64",
      "userId": "05947012679519155640"
     },
     "user_tz": -120
    },
    "id": "L2oXeouoWaf9",
    "outputId": "50acee0e-d94e-454a-9ec0-aeb166409ae0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'El', 'presidente', 'de', 'la', 'rep√∫blica', 'es', 'J', '##uan', 'P√©rez', ',', 'que', 'fue', 'nombrado', 'en', '1992', '.', '[SEP]']\n",
      "QuestionAnsweringModelOutput(loss=tensor(3.3530, grad_fn=<DivBackward0>), start_logits=tensor([[0.5718, 0.5167, 0.2103, 0.5927, 0.4087, 0.4196, 0.3866, 0.3108, 0.7489,\n",
      "         0.5119, 0.5294, 0.7897, 0.4597, 0.3280, 0.7372, 0.3749, 0.4290, 0.1096,\n",
      "         0.4879, 0.7059, 0.6806, 0.3969, 0.4912, 0.0961, 0.4025, 0.7489]],\n",
      "       grad_fn=<SqueezeBackward1>), end_logits=tensor([[ 0.0274,  0.0849,  0.2804, -0.1664,  0.0802, -0.0723,  0.1448,  0.0878,\n",
      "         -0.2123,  0.0080,  0.0852,  0.0074, -0.1405,  0.0426,  0.0194, -0.0530,\n",
      "          0.0022, -0.0888,  0.2044,  0.1623,  0.1322,  0.1192, -0.0710, -0.2225,\n",
      "         -0.1345, -0.2124]], grad_fn=<SqueezeBackward1>), hidden_states=None, attentions=None)\n"
     ]
    }
   ],
   "source": [
    "question, text = \"¬øC√≥mo se llama el presidente?\", \"El presidente de la rep√∫blica es Juan P√©rez, que fue nombrado en 1992.\"\n",
    "inputs = tokenizer(question, text, return_tensors='pt')\n",
    "\n",
    "#print(input_ids)\n",
    "tokens_answ = tokenizer.convert_ids_to_tokens(tokenizer.encode(text))\n",
    "print(tokens_answ)\n",
    "\n",
    "#Juan P√©rez\n",
    "start_positions = torch.tensor([7])\n",
    "end_positions = torch.tensor([9])\n",
    "\n",
    "output = model3(**inputs, start_positions=start_positions, end_positions=end_positions)\n",
    "\n",
    "output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1098,
     "status": "ok",
     "timestamp": 1617969389473,
     "user": {
      "displayName": "Rafael Berlanga Llavori",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gij7a_x7hln_mccXdSKtg2BpPtMu66FEoe7m3u32g=s64",
      "userId": "05947012679519155640"
     },
     "user_tz": -120
    },
    "id": "GX9ChcazcxGV",
    "outputId": "f6416333-d364-4724-e03f-5cfd19acb1cb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(11) tensor(2)\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "# El segmento a extraer para la respuesta ser√≠a\n",
    "\n",
    "start_scores = torch.argmax(output.start_logits)\n",
    "end_scores   = torch.argmax(output.end_logits)\n",
    "\n",
    "print(start_scores, end_scores)\n",
    "print(tokens_answ[start_scores: end_scores + 1]) # vac√≠o ya que el final es menor que el principio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YuINSTxU8eMD"
   },
   "source": [
    "No da respuesta porque el final est√° antes que el principio. El modelo de base no ha sido entrenado para esa tarea y por tanto debe ajustarse con ejemplos concretos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rwFrJ9-ye7Xu"
   },
   "source": [
    "**CONCLUSI√ìN**: Todos estos modelos deben ser entrenados en la tarea concreta (\"fine-tuning\") para obtener los resultados esperados.\n",
    "Para ello se recomienda utilizar una librer√≠a m√°s simple enfocada a tareas concretas, como \"SimpleTransformers\""
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "BETO el Transfomer en Espa√±ol",
   "provenance": [
    {
     "file_id": "1uRwg4UmPgYIqGYY4gW_Nsw9782GFJbPt",
     "timestamp": 1581328769878
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
